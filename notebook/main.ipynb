{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import sys\n",
    "sys.path.append('../src')\n",
    "\n",
    "import qa\n",
    "importlib.reload(qa)\n",
    "import qa\n",
    "\n",
    "import plot\n",
    "importlib.reload(plot)\n",
    "import plot\n",
    "\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import re\n",
    "import random\n",
    "import ast\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import scipy.stats as stats\n",
    "from scipy.stats import ttest_rel\n",
    "from scipy.stats import pearsonr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = '../data/MMLU-Pro/sample_results/mmlu_list_ad.pkl'\n",
    "results = pickle.load(open(PATH, 'rb'))\n",
    "accuracy = [r['r^org']==r['answer'] for r in results]\n",
    "conf = [r['p_r^org'] for r in results]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = '../data/MMLU-Pro/sample_results/mmlu_list_da.pkl'\n",
    "plot_from_file(PATH, metric = 'consistency', vlimit=(0,1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MMLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jch0/.conda/envs/jch0/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Downloading readme: 100%|██████████| 10.9k/10.9k [00:00<00:00, 7.77MB/s]\n",
      "Downloading data: 100%|██████████| 4.16M/4.16M [00:00<00:00, 18.7MB/s]\n",
      "Downloading data: 100%|██████████| 45.3k/45.3k [00:00<00:00, 624kB/s]\n",
      "Generating test split: 100%|██████████| 12032/12032 [00:00<00:00, 152714.45 examples/s]\n",
      "Generating validation split: 100%|██████████| 70/70 [00:00<00:00, 19347.70 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total number of questions in MMLU-Pro: 12032\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing MMLU: 100%|██████████| 12032/12032 [03:19<00:00, 60.35it/s] \n"
     ]
    }
   ],
   "source": [
    "# from datasets import load_dataset\n",
    "\n",
    "# ds = load_dataset(\"TIGER-Lab/MMLU-Pro\")\n",
    "\n",
    "# mmlu = {}\n",
    "# for ele in ds['test']:\n",
    "#     if ele['category'] not in mmlu:\n",
    "#         mmlu[ele['category']] = []\n",
    "#     mmlu[ele['category']].append(ele)\n",
    "\n",
    "# import random\n",
    "\n",
    "# # mmlu_samples = []\n",
    "# # for cat in mmlu:\n",
    "# #     mmlu_samples.extend(random.Random(0).sample(mmlu[cat], 10))\n",
    "\n",
    "# # with open('../data/mmlu_samples_140.pkl', 'wb') as f:\n",
    "# #     pickle.dump(mmlu_samples, f)\n",
    "\n",
    "# mmlu_full = []\n",
    "# for cat in mmlu:\n",
    "#     mmlu_full.extend(mmlu[cat])\n",
    "    \n",
    "# with open('../data/MMLU-Pro/results/mmlu_full.pkl', 'wb') as f:\n",
    "#     pickle.dump(mmlu_full, f)\n",
    "\n",
    "# print(\"total number of questions in MMLU-Pro:\", len(mmlu_full))\n",
    "\n",
    "# res_oqa_org = qa.qa_eval_org(mmlu_full)\n",
    "\n",
    "# with open('../data/MMLU-Pro/results/mmlu_org.pkl', 'wb') as f:\n",
    "#     pickle.dump(res_oqa_org, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing MMLU: 100%|██████████| 12032/12032 [03:22<00:00, 59.41it/s] \n",
      "Processing MMLU: 100%|██████████| 12032/12032 [03:21<00:00, 59.66it/s] \n",
      "Processing MMLU: 100%|██████████| 12032/12032 [03:28<00:00, 57.78it/s] \n",
      "Processing MMLU: 100%|██████████| 12032/12032 [03:22<00:00, 59.41it/s] \n"
     ]
    }
   ],
   "source": [
    "# with open('../data/MMLU-Pro/results/mmlu_org.pkl', 'rb') as f:\n",
    "#     res_mmlu_org = pickle.load(f)\n",
    "\n",
    "# type_names = ['1st', '2nd', 'rnd', 'lst']\n",
    "# for disagree_type in type_names:\n",
    "#     results = qa.qa_eval_one(res_mmlu_org, disagree_type)\n",
    "\n",
    "#     with open(f'../data/MMLU-Pro/results/mmlu_one_{disagree_type}.pkl', 'wb') as f:\n",
    "#         pickle.dump(results, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing grp_list samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing QA: 100%|██████████| 16940/16940 [04:25<00:00, 63.74it/s] \n"
     ]
    }
   ],
   "source": [
    "with open('../data/MMLU-Pro/sample_results/mmlu_org.pkl', 'rb') as f:\n",
    "    res_mmlu_org = pickle.load(f)\n",
    "\n",
    "\n",
    "input_feat_list = []\n",
    "for disagree_size in range(11): # row\n",
    "    input_row = []\n",
    "    for agree_size in range(11): # column\n",
    "        \n",
    "        eval_feat = {\n",
    "            'type': 'grp_list',\n",
    "            'agree_size': agree_size,\n",
    "            'disagree_size': disagree_size,\n",
    "            'disagree_type': '2nd',\n",
    "            'q_type': 'factual',\n",
    "            'order': 'da'\n",
    "        }\n",
    "        input_row.append(eval_feat)\n",
    "    input_feat_list.append(input_row)\n",
    "\n",
    "results, accuracy = qa.qa_eval_matrix(res_mmlu_org, input_feat_list)\n",
    "\n",
    "with open('../data/MMLU-Pro/sample_results/mmlu_list_da.pkl', 'wb') as f:\n",
    "    pickle.dump(results, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/MMLU-Pro/sample_results/res_mmlu_org.pkl', 'rb') as f:\n",
    "    res_mmlu_org = pickle.load(f)\n",
    "\n",
    "\n",
    "input_feat_list = []\n",
    "for group_size in [4,12,50,100,1000]: # row\n",
    "    input_row = []\n",
    "    for agree_ratio in [0, 0.25, 0.50, 0.75, 1]: # column\n",
    "        \n",
    "        eval_feat = {\n",
    "            'type': 'grp_ratio',\n",
    "            'group_size': group_size,\n",
    "            'disagree_ratio': 1-agree_ratio,\n",
    "            'disagree_type': '2nd',\n",
    "            'q_type': 'factual',\n",
    "            'order': 'ad'\n",
    "        }\n",
    "        input_row.append(eval_feat)\n",
    "    input_feat_list.append(input_row)\n",
    "\n",
    "results, accuracy = qa.qa_eval_matrix(res_mmlu_org, input_feat_list)\n",
    "    #     result, accuracy = qa.qa_eval_matrix(res_mmlu_org, eval_feat)\n",
    "    #     output_row.append(accuracy)\n",
    "    # output_list.append(output_row)\n",
    "\n",
    "# round a 2d np array\n",
    "accuracy = np.array([[round(ele,2) for ele in row] for row in accuracy])\n",
    "\n",
    "with open('../data/MMLU-Pro/sample_results/mmlu_ratio_ad.pkl', 'wb') as f:\n",
    "    pickle.dump(results, f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reason Generation and Reason in-context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating reasons: 100%|██████████| 2800/2800 [05:15<00:00,  8.86it/s]\n"
     ]
    }
   ],
   "source": [
    "# with open('../data/MMLU-Pro/sample_results/res_mmlu_org.pkl', 'rb') as f:\n",
    "#     res_mmlu_org = pickle.load(f)\n",
    "\n",
    "# mmlu_org_reason = qa.qa_generate_reason(res_mmlu_org, '2nd', 10)\n",
    "\n",
    "# with open('../data/MMLU-Pro/sample_results/mmlu_org_reason.pkl', 'wb') as f:\n",
    "#     pickle.dump(mmlu_org_reason, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing grp_discrete samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing QA: 100%|██████████| 16940/16940 [06:27<00:00, 43.67it/s] \n"
     ]
    }
   ],
   "source": [
    "with open('../data/MMLU-Pro/sample_results/mmlu_org_reason.pkl', 'rb') as f:\n",
    "    mmlu_org_reason = pickle.load(f)\n",
    "\n",
    "input_feat_list = []\n",
    "for disagree_size in range(11): # row\n",
    "    input_row = []\n",
    "    for agree_size in range(11): # column\n",
    "        \n",
    "        eval_feat = {\n",
    "            'type': 'grp_disc',\n",
    "            'use_reason': True,\n",
    "            'agree_size': agree_size,\n",
    "            'disagree_size': disagree_size,\n",
    "            'disagree_type': '2nd',\n",
    "            'q_type': 'factual',\n",
    "            'order': 'da'\n",
    "        }\n",
    "        input_row.append(eval_feat)\n",
    "    input_feat_list.append(input_row)\n",
    "\n",
    "results, accuracy = qa.qa_eval_matrix(mmlu_org_reason, input_feat_list)\n",
    "\n",
    "# round a 2d np array\n",
    "accuracy = np.array([[round(ele,2) for ele in row] for row in accuracy])\n",
    "\n",
    "with open('../data/MMLU-Pro/sample_results/mmlu_reason_da.pkl', 'wb') as f:\n",
    "    pickle.dump(results, f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### bar plot for self-confidence vs consistency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Load data\n",
    "PATH = '../data/MMLU-Pro/results/mmlu_one_2nd.pkl'\n",
    "with open(PATH, 'rb') as f:\n",
    "    results = pickle.load(f)\n",
    "\n",
    "# Extract accuracy and confidence\n",
    "accuracy = np.array([r['r'] == r['answer'] for r in results])\n",
    "consistency = np.array([r['r'] == r['r^org'] for r in results])\n",
    "self_conf = np.array([r['p_r^org'] for r in results])\n",
    "other_conf = np.array([r['p_r_j'] for r in results])\n",
    "\n",
    "# Bin confidence scores into 10 bins from 0 to 1\n",
    "bins = np.linspace(0, 1, 11)\n",
    "bin_indices = np.digitize(self_conf, bins) - 1  # Adjust bin index to start at 0\n",
    "\n",
    "# Calculate average accuracy for each bin\n",
    "bin_accuracy = []\n",
    "bin_centers = []\n",
    "\n",
    "for i in range(10):\n",
    "    indices = bin_indices == i\n",
    "    if np.any(indices):\n",
    "        avg_acc = consistency[indices].mean()\n",
    "    else:\n",
    "        avg_acc = np.nan\n",
    "    bin_accuracy.append(avg_acc)\n",
    "    bin_centers.append((bins[i] + bins[i+1]) / 2)\n",
    "\n",
    "# Plot the bar chart\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.bar(bin_centers, bin_accuracy, width=0.09, edgecolor='black')\n",
    "plt.xlabel('Self-Confidence')\n",
    "plt.ylabel('Average Consistency')\n",
    "plt.title('Consistency vs Self-Confidence')\n",
    "plt.xticks(np.round(bin_centers, 2))\n",
    "plt.ylim(0, 1)\n",
    "plt.grid(True, linestyle='--', alpha=0.6)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### in/out-domain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing QA: 100%|██████████| 24064/24064 [05:37<00:00, 71.21it/s] \n"
     ]
    }
   ],
   "source": [
    "# with open('../data/MMLU-Pro/results/mmlu_org.pkl', 'rb') as f:\n",
    "#     res_mmlu_org = pickle.load(f)\n",
    "\n",
    "# results = qa.qa_eval_domain(res_mmlu_org, '2nd')\n",
    "\n",
    "# with open(f'../data/MMLU-Pro/results/mmlu_domain_2nd.pkl', 'wb') as f:\n",
    "#     pickle.dump(results, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total number of domains in MMLU-Pro: 14\n",
      "domains in MMLU-Pro: {'math', 'engineering', 'physics', 'computer science', 'chemistry', 'psychology', 'law', 'economics', 'philosophy', 'history', 'biology', 'other', 'business', 'health'}\n"
     ]
    }
   ],
   "source": [
    "# domains = set()\n",
    "# for ele in res_mmlu_org:\n",
    "#     domains.add(ele['category'])\n",
    "\n",
    "# print(\"total number of domains in MMLU-Pro:\", len(domains))\n",
    "# print(\"domains in MMLU-Pro:\", domains)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/MMLU-Pro/results/mmlu_domain_2nd.pkl', 'rb') as f:\n",
    "    results = pickle.load(f)\n",
    "\n",
    "with open('../data/MMLU-Pro/results/mmlu_one_2nd.pkl', 'rb') as f:\n",
    "    res_mmlu_one = pickle.load(f)\n",
    "\n",
    "results_in_domain = results['in_domain']\n",
    "results_out_domain = results['out_domain']\n",
    "results_control = res_mmlu_one\n",
    "\n",
    "consistence_in_domain  = np.array([r['r'] == r['r^org'] for r in results_in_domain])\n",
    "consistence_out_domain   = np.array([r['r'] == r['r^org'] for r in results_out_domain])\n",
    "consistence_control = np.array([r['r'] == r['r^org'] for r in results_control])\n",
    "\n",
    "# Calculate means and confidence intervals\n",
    "domain_groups = [consistence_in_domain, consistence_out_domain, consistence_control]\n",
    "mean_consistance = [group.mean() for group in domain_groups]\n",
    "confidence_intervals = [stats.t.interval(\n",
    "    0.95, len(group)-1, loc=group.mean(), scale=stats.sem(group)) for group in domain_groups]\n",
    "\n",
    "# Extracting lower and upper bounds for plotting\n",
    "ci_lower = [mean - ci[0] for mean, ci in zip(mean_consistance, confidence_intervals)]\n",
    "ci_upper = [ci[1] - mean for mean, ci in zip(mean_consistance, confidence_intervals)]\n",
    "\n",
    "in_out_domain = ['In-Domain', 'Out-Domain', 'Control']\n",
    "\n",
    "# Plotting with confidence intervals\n",
    "plt.figure(figsize=(4, 6))\n",
    "plt.bar(in_out_domain, mean_consistance, yerr=[ci_lower, ci_upper], capsize=10,\n",
    "        color=['skyblue', 'lightgreen', 'lightgray'], alpha=0.7)\n",
    "plt.ylabel('Consistency (0-1)')\n",
    "plt.xlabel('Expertise')\n",
    "plt.title('Consistency by Expertise \\nwith 95% Confidence Intervals')\n",
    "plt.ylim(0, 1)\n",
    "\n",
    "# Display values on the bars\n",
    "for i, val in enumerate(mean_consistance):\n",
    "    plt.text(i, val + max(ci_upper)*1, f'{val:.2f}', ha='center', va='bottom')\n",
    "\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing QA batch 1: 100%|██████████| 1000/1000 [00:20<00:00, 48.11it/s]\n",
      "Processing QA batch 2: 100%|██████████| 1000/1000 [00:21<00:00, 47.11it/s]\n",
      "Processing QA batch 3: 100%|██████████| 1000/1000 [00:23<00:00, 42.29it/s]\n",
      "Processing QA batch 4: 100%|██████████| 1000/1000 [00:18<00:00, 55.16it/s]\n",
      "Processing QA batch 5: 100%|██████████| 1000/1000 [00:18<00:00, 54.65it/s]\n",
      "Processing QA batch 6: 100%|██████████| 1000/1000 [00:20<00:00, 49.19it/s]\n",
      "Processing QA batch 7: 100%|██████████| 1000/1000 [00:18<00:00, 52.70it/s]\n",
      "Processing QA batch 8: 100%|██████████| 1000/1000 [00:18<00:00, 54.74it/s]\n",
      "Processing QA batch 9: 100%|██████████| 1000/1000 [00:22<00:00, 45.27it/s]\n",
      "Processing QA batch 10: 100%|██████████| 1000/1000 [00:21<00:00, 47.21it/s]\n",
      "Processing QA batch 11: 100%|██████████| 1000/1000 [00:22<00:00, 43.64it/s]\n",
      "Processing QA batch 12: 100%|██████████| 1000/1000 [00:26<00:00, 38.16it/s]\n",
      "Processing QA batch 13: 100%|██████████| 1000/1000 [00:23<00:00, 43.03it/s]\n",
      "Processing QA batch 14: 100%|██████████| 1000/1000 [00:23<00:00, 42.49it/s]\n",
      "Processing QA batch 15: 100%|██████████| 1000/1000 [00:53<00:00, 18.61it/s]\n",
      "Processing QA batch 16: 100%|██████████| 1000/1000 [00:22<00:00, 43.83it/s]\n",
      "Processing QA batch 17: 100%|██████████| 1000/1000 [00:22<00:00, 44.93it/s]\n",
      "Processing QA batch 18: 100%|██████████| 1000/1000 [01:00<00:00, 16.59it/s]\n",
      "Processing QA batch 19: 100%|██████████| 1000/1000 [00:18<00:00, 52.81it/s]\n",
      "Processing QA batch 20: 100%|██████████| 1000/1000 [00:20<00:00, 49.20it/s]\n",
      "Processing QA batch 21: 100%|██████████| 1000/1000 [00:21<00:00, 47.54it/s]\n",
      "Processing QA batch 22: 100%|██████████| 1000/1000 [00:19<00:00, 52.11it/s]\n",
      "Processing QA batch 23: 100%|██████████| 1000/1000 [00:19<00:00, 52.48it/s]\n",
      "Processing QA batch 24: 100%|██████████| 1000/1000 [00:19<00:00, 52.50it/s]\n",
      "Processing QA batch 25: 100%|██████████| 64/64 [00:01<00:00, 34.20it/s]\n"
     ]
    }
   ],
   "source": [
    "with open('../data/MMLU-Pro/results/org.pkl', 'rb') as f:\n",
    "    res_org = pickle.load(f)\n",
    "\n",
    "results = qa.qa_eval_hierarchy(res_org, '2nd')\n",
    "\n",
    "with open(f'../data/MMLU-Pro/results/hierarchy_2nd.pkl', 'wb') as f:\n",
    "    pickle.dump(results, f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the dataset since Idavidrein/gpqa couldn't be found on the Hugging Face Hub\n",
      "Found the latest cached dataset configuration 'gpqa_diamond' at /home/jch0/.cache/huggingface/datasets/Idavidrein___gpqa/gpqa_diamond/0.0.0/90b8e5be2b1d3d2dbfe016cdab47981150600c4a (last modified on Wed Apr 30 08:52:17 2025).\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Login using e.g. `huggingface-cli login` to access this dataset\n",
    "hf_token = input(\"Enter your Hugging Face token: \")\n",
    "ds = load_dataset(\"Idavidrein/gpqa\", \"gpqa_diamond\", token = hf_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total number of questions in GPQA: 198\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing QA: 100%|██████████| 198/198 [00:04<00:00, 41.37it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "gpqa_full = []\n",
    "for q_idx, ele in enumerate(ds['train']):\n",
    "    input_ele = {}\n",
    "    input_ele['question_id'] = q_idx\n",
    "    input_ele['category'] = ele['High-level domain']\n",
    "    input_ele['question'] = ele['Question']\n",
    "    choices = [ele['Correct Answer'], ele['Incorrect Answer 1'], ele['Incorrect Answer 2'], ele['Incorrect Answer 3']]\n",
    "    random.Random(q_idx).shuffle(choices)\n",
    "    input_ele['options'] = choices\n",
    "    input_ele['answer'] = 'ABCD'[choices.index(ele['Correct Answer'])]\n",
    "\n",
    "    gpqa_full.append(input_ele)\n",
    "\n",
    "with open('../data/GPQA/results/full.pkl', 'wb') as f:\n",
    "    pickle.dump(gpqa_full, f)\n",
    "\n",
    "print(\"total number of questions in GPQA:\", len(gpqa_full))\n",
    "\n",
    "res_gpqa_org = qa.qa_eval_org(gpqa_full)\n",
    "\n",
    "with open('../data/GPQA/results/org.pkl', 'wb') as f:\n",
    "    pickle.dump(res_gpqa_org, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing QA: 100%|██████████| 198/198 [00:04<00:00, 41.42it/s]\n",
      "Processing QA: 100%|██████████| 198/198 [00:03<00:00, 56.86it/s]\n",
      "Processing QA: 100%|██████████| 198/198 [00:06<00:00, 30.46it/s]\n"
     ]
    }
   ],
   "source": [
    "with open('../data/GPQA/results/gpqa_org.pkl', 'rb') as f:\n",
    "    res_gpqa_org = pickle.load(f)\n",
    "\n",
    "type_names = ['1st', '2nd', 'rnd', 'lst']\n",
    "\n",
    "for disagree_type in type_names:\n",
    "    results = qa.qa_eval_one(res_gpqa_org, disagree_type)\n",
    "    with open(f'../data/GPQA/results/gpqa_one_{disagree_type}.pkl', 'wb') as f:\n",
    "        pickle.dump(results, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing QA: 100%|██████████| 396/396 [00:07<00:00, 53.61it/s]\n"
     ]
    }
   ],
   "source": [
    "with open('../data/GPQA-Diamond/results/org.pkl', 'rb') as f:\n",
    "    res_org = pickle.load(f)\n",
    "\n",
    "results = qa.qa_eval_domain(res_org, '2nd')\n",
    "\n",
    "with open(f'../data/GPQA-Diamond/results/domain_2nd.pkl', 'wb') as f:\n",
    "    pickle.dump(results, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing QA batch 1: 100%|██████████| 396/396 [00:08<00:00, 45.27it/s]\n"
     ]
    }
   ],
   "source": [
    "with open('../data/GPQA-Diamond/results/org.pkl', 'rb') as f:\n",
    "    res_org = pickle.load(f)\n",
    "\n",
    "results = qa.qa_eval_hierarchy(res_org, '2nd')\n",
    "\n",
    "with open(f'../data/GPQA-Diamond/results/hierarchy_2nd.pkl', 'wb') as f:\n",
    "    pickle.dump(results, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ARC-Challenge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Login using e.g. `huggingface-cli login` to access this dataset\n",
    "ds = load_dataset(\"allenai/ai2_arc\", \"ARC-Challenge\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total number of questions in ARC: 1172\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing QA: 100%|██████████| 1172/1172 [00:16<00:00, 71.50it/s] \n"
     ]
    }
   ],
   "source": [
    "\n",
    "arc_full = []\n",
    "for q_idx, ele in enumerate(ds['test']):\n",
    "    input_ele = {}\n",
    "    input_ele['question_id'] = ele['id']\n",
    "    # input_ele['category'] = ele['High-level domain']\n",
    "    input_ele['question'] = ele['question']\n",
    "    input_ele['options'] = ele[\"choices\"][\"text\"]\n",
    "    answer_key = ele[\"answerKey\"]\n",
    "    if answer_key not in \"ABCDE\":\n",
    "        answer_key = \"ABCDE\"[int(answer_key)-1]\n",
    "    input_ele['answer'] = answer_key\n",
    "\n",
    "    arc_full.append(input_ele)\n",
    "\n",
    "with open('../data/ARC/results/full.pkl', 'wb') as f:\n",
    "    pickle.dump(arc_full, f)\n",
    "\n",
    "print(\"total number of questions in ARC:\", len(arc_full))\n",
    "\n",
    "res_arc_org = qa.qa_eval_org(arc_full)\n",
    "\n",
    "with open('../data/ARC/results/org.pkl', 'wb') as f:\n",
    "    pickle.dump(res_arc_org, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing QA: 100%|██████████| 1172/1172 [00:16<00:00, 69.54it/s]\n",
      "Processing QA: 100%|██████████| 1172/1172 [00:16<00:00, 70.93it/s]\n",
      "Processing QA: 100%|██████████| 1172/1172 [00:15<00:00, 75.63it/s] \n",
      "Processing QA: 100%|██████████| 1172/1172 [00:15<00:00, 73.90it/s] \n"
     ]
    }
   ],
   "source": [
    "with open('../data/ARC/results/org.pkl', 'rb') as f:\n",
    "    res_org = pickle.load(f)\n",
    "\n",
    "type_names = ['1st', '2nd', 'rnd', 'lst']\n",
    "\n",
    "for disagree_type in type_names:\n",
    "    results = qa.qa_eval_one(res_org, disagree_type)\n",
    "    with open(f'../data/ARC/results/one_{disagree_type}.pkl', 'wb') as f:\n",
    "        pickle.dump(results, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing QA batch 1: 100%|██████████| 1000/1000 [00:21<00:00, 46.58it/s]\n",
      "Processing QA batch 2: 100%|██████████| 1000/1000 [00:21<00:00, 46.84it/s]\n",
      "Processing QA batch 3: 100%|██████████| 344/344 [00:07<00:00, 44.66it/s]\n"
     ]
    }
   ],
   "source": [
    "with open('../data/ARC/results/org.pkl', 'rb') as f:\n",
    "    res_org = pickle.load(f)\n",
    "\n",
    "results = qa.qa_eval_hierarchy(res_org, '2nd')\n",
    "\n",
    "with open(f'../data/ARC/results/hierarchy_2nd.pkl', 'wb') as f:\n",
    "    pickle.dump(results, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OpinionQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total number of questions in OpinionQA: 1506\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing QA: 100%|██████████| 1506/1506 [00:38<00:00, 39.38it/s]\n"
     ]
    }
   ],
   "source": [
    "oqa_df = pd.DataFrame()\n",
    "# get all file names under PATH\n",
    "import os\n",
    "PATH = '../data/OpinionQA/model_input/'\n",
    "for file in os.listdir(PATH):\n",
    "    if re.match(r'.+W\\d{2}\\.csv', file):\n",
    "        target_df = pd.read_csv(PATH + file, delimiter='\\t')\n",
    "        target_df = target_df[[\"key\", \"question\", \"options\"]]\n",
    "        target_df.columns = [\"question_id\", \"question\", \"options\"]\n",
    "        oqa_df = pd.concat([oqa_df, target_df], ignore_index=True)\n",
    "\n",
    "question_id = oqa_df['question_id'].tolist()\n",
    "question = oqa_df['question'].tolist()\n",
    "options = oqa_df['options'].tolist()\n",
    "options = [ast.literal_eval(x) for x in options]\n",
    "\n",
    "for ele in options:\n",
    "    if 'Refused' in ele:\n",
    "        ele.remove('Refused')\n",
    "\n",
    "oqa_list = []\n",
    "for i in range(len(question_id)):\n",
    "    oqa_list.append({\"question_id\": question_id[i], \"question\": question[i], \"options\": options[i]})\n",
    "\n",
    "print(\"total number of questions in OpinionQA:\", len(oqa_list))\n",
    "# oqa_sample_list = random.Random(0).sample(oqa_list, 140)\n",
    "# print(\"total number of questions in OpinionQA sample:\", len(oqa_sample_list))\n",
    "\n",
    "# res_oqa_org = qa.qa_eval_org(oqa_sample_list)\n",
    "\n",
    "# with open('../data/OpinionQA/sample_results/res_oqa_org.pkl', 'wb') as f:\n",
    "#     pickle.dump(res_oqa_org, f)\n",
    "\n",
    "res_oqa_org = qa.qa_eval_org(oqa_list)\n",
    "\n",
    "with open('../data/OpinionQA/results/org.pkl', 'wb') as f:\n",
    "    pickle.dump(res_oqa_org, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing QA: 100%|██████████| 1506/1506 [00:27<00:00, 54.83it/s]\n",
      "Processing QA: 100%|██████████| 1506/1506 [00:24<00:00, 62.67it/s]\n",
      "Processing QA: 100%|██████████| 1506/1506 [00:24<00:00, 61.37it/s] \n",
      "Processing QA: 100%|██████████| 1506/1506 [00:23<00:00, 64.19it/s] \n"
     ]
    }
   ],
   "source": [
    "with open('../data/OpinionQA/results/org.pkl', 'rb') as f:\n",
    "    res_oqa_org = pickle.load(f)\n",
    "\n",
    "type_names = ['1st', '2nd', 'rnd', 'lst']\n",
    "\n",
    "for disagree_type in type_names:\n",
    "    results = qa.qa_eval_one(res_oqa_org, disagree_type)\n",
    "    with open(f'../data/OpinionQA/results/one_{disagree_type}.pkl', 'wb') as f:\n",
    "        pickle.dump(results, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing grp_count samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing MMLU: 100%|██████████| 16940/16940 [04:57<00:00, 56.99it/s] \n"
     ]
    }
   ],
   "source": [
    "with open('../data/OpinionQA/sample_results/res_oqa_org.pkl', 'rb') as f:\n",
    "    res_oqa_org = pickle.load(f)\n",
    "\n",
    "\n",
    "input_feat_list = []\n",
    "for disagree_size in range(11): # row\n",
    "    input_row = []\n",
    "    for agree_size in range(11): # column\n",
    "        \n",
    "        eval_feat = {\n",
    "            'type': 'grp_count',\n",
    "            'agree_size': agree_size,\n",
    "            'disagree_size': disagree_size,\n",
    "            'disagree_type': '2nd',\n",
    "            'order': 'ad',\n",
    "            'q_type': 'opinion'\n",
    "        }\n",
    "        input_row.append(eval_feat)\n",
    "    input_feat_list.append(input_row)\n",
    "\n",
    "results, accuracy = qa.qa_eval_matrix(res_oqa_org, input_feat_list)\n",
    "\n",
    "# round a 2d np array\n",
    "accuracy = np.array([[round(ele,2) for ele in row] for row in accuracy])\n",
    "\n",
    "with open('../data/OpinionQA/sample_results/oqa_count_ad.pkl', 'wb') as f:\n",
    "    pickle.dump(results, f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reason Generation and Reason in-context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating reasons: 100%|██████████| 2800/2800 [02:23<00:00, 19.56it/s]\n"
     ]
    }
   ],
   "source": [
    "with open('../data/OpinionQA/sample_results/oqa_org.pkl', 'rb') as f:\n",
    "    res_oqa_org = pickle.load(f)\n",
    "\n",
    "oqa_org_reason = qa.qa_generate_reason(res_oqa_org, '2nd', 10)\n",
    "\n",
    "with open('../data/OpinionQA/sample_results/oqa_org_reason.pkl', 'wb') as f:\n",
    "    pickle.dump(oqa_org_reason, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing grp_disc samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing QA: 100%|██████████| 16940/16940 [05:27<00:00, 51.78it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing grp_disc samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing QA: 100%|██████████| 16940/16940 [05:18<00:00, 53.18it/s] \n"
     ]
    }
   ],
   "source": [
    "with open('../data/OpinionQA/sample_results/oqa_org_reason.pkl', 'rb') as f:\n",
    "    oqa_org_reason = pickle.load(f)\n",
    "\n",
    "for order in ['ad', 'da']:\n",
    "    input_feat_list = []\n",
    "    for disagree_size in range(11): # row\n",
    "        input_row = []\n",
    "        for agree_size in range(11): # column\n",
    "            \n",
    "            eval_feat = {\n",
    "                'type': 'grp_disc',\n",
    "                'use_reason': False,\n",
    "                'agree_size': agree_size,\n",
    "                'disagree_size': disagree_size,\n",
    "                'disagree_type': '2nd',\n",
    "                'q_type': 'opinion',\n",
    "                'order': order\n",
    "            }\n",
    "            input_row.append(eval_feat)\n",
    "        input_feat_list.append(input_row)\n",
    "\n",
    "    results, accuracy = qa.qa_eval_matrix(mmlu_org_reason, input_feat_list)\n",
    "\n",
    "    # round a 2d np array\n",
    "    accuracy = np.array([[round(ele,2) for ele in row] for row in accuracy])\n",
    "\n",
    "    with open(f'../data/OpinionQA/sample_results/oqa_disc_{order}.pkl', 'wb') as f:\n",
    "        pickle.dump(results, f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing QA batch 1: 100%|██████████| 1000/1000 [00:21<00:00, 46.52it/s]\n",
      "Processing QA batch 2: 100%|██████████| 1000/1000 [00:19<00:00, 50.04it/s]\n",
      "Processing QA batch 3: 100%|██████████| 1000/1000 [00:21<00:00, 46.43it/s]\n",
      "Processing QA batch 4: 100%|██████████| 12/12 [00:01<00:00, 10.78it/s]\n"
     ]
    }
   ],
   "source": [
    "with open('../data/OpinionQA/results/org.pkl', 'rb') as f:\n",
    "    res_org = pickle.load(f)\n",
    "\n",
    "results = qa.qa_eval_hierarchy(res_org, '2nd')\n",
    "\n",
    "with open(f'../data/OpinionQA/results/hierarchy_2nd.pkl', 'wb') as f:\n",
    "    pickle.dump(results, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SIQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total number of questions in SIQA: 1954\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing QA: 100%|██████████| 1954/1954 [00:26<00:00, 74.18it/s] \n"
     ]
    }
   ],
   "source": [
    "# load jsonl\n",
    "import json\n",
    "PATH = '../data/SIQA/raw/socialiqa-train-dev/dev.jsonl'\n",
    "answer_PATH = '../data/SIQA/raw/socialiqa-train-dev/dev-labels.lst'\n",
    "with open(PATH, 'r') as f:\n",
    "    siqa_list = [json.loads(line) for line in f]\n",
    "\n",
    "with open(answer_PATH, 'r') as f:\n",
    "    answers = [line.strip() for line in f]\n",
    "\n",
    "siqa_full = []\n",
    "for i, ele in enumerate(siqa_list):\n",
    "    input_ele = {}\n",
    "    input_ele['question_id'] = i\n",
    "    input_ele['question'] = ele['context'] + ' ' + ele['question']\n",
    "    input_ele['options'] = [ele['answerA'], ele['answerB'], ele['answerC']]\n",
    "    input_ele['answer'] = \"ABC\"[int(answers[i])-1]\n",
    "\n",
    "    siqa_full.append(input_ele)\n",
    "\n",
    "with open('../data/SIQA/results/full.pkl', 'wb') as f:\n",
    "    pickle.dump(siqa_full, f)\n",
    "\n",
    "print(\"total number of questions in SIQA:\", len(siqa_full))\n",
    "\n",
    "res_org = qa.qa_eval_org(siqa_full)\n",
    "\n",
    "with open('../data/SIQA/results/org.pkl', 'wb') as f:\n",
    "    pickle.dump(res_org, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing QA: 100%|██████████| 1954/1954 [00:31<00:00, 62.21it/s] \n",
      "Processing QA: 100%|██████████| 1954/1954 [00:29<00:00, 66.36it/s] \n",
      "Processing QA: 100%|██████████| 1954/1954 [00:27<00:00, 71.68it/s] \n",
      "Processing QA: 100%|██████████| 1954/1954 [00:27<00:00, 72.22it/s] \n"
     ]
    }
   ],
   "source": [
    "with open('../data/SIQA/results/org.pkl', 'rb') as f:\n",
    "    res_org = pickle.load(f)\n",
    "\n",
    "type_names = ['1st', '2nd', 'rnd', 'lst']\n",
    "\n",
    "for disagree_type in type_names:\n",
    "    results = qa.qa_eval_one(res_org, disagree_type)\n",
    "    with open(f'../data/SIQA/results/one_{disagree_type}.pkl', 'wb') as f:\n",
    "        pickle.dump(results, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing QA batch 1: 100%|██████████| 1000/1000 [00:19<00:00, 51.07it/s]\n",
      "Processing QA batch 2: 100%|██████████| 1000/1000 [00:21<00:00, 47.21it/s]\n",
      "Processing QA batch 3: 100%|██████████| 1000/1000 [00:19<00:00, 51.67it/s]\n",
      "Processing QA batch 4: 100%|██████████| 908/908 [00:18<00:00, 48.40it/s]\n"
     ]
    }
   ],
   "source": [
    "with open('../data/SIQA/results/org.pkl', 'rb') as f:\n",
    "    res_org = pickle.load(f)\n",
    "\n",
    "results = qa.qa_eval_hierarchy(res_org, '2nd')\n",
    "\n",
    "with open(f'../data/SIQA/results/hierarchy_2nd.pkl', 'wb') as f:\n",
    "    pickle.dump(results, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GlobalOpinionQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"Anthropic/llm_global_opinions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total number of questions in GlobalOpinionQA: 2555\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing QA: 100%|██████████| 2555/2555 [00:42<00:00, 60.78it/s] \n"
     ]
    }
   ],
   "source": [
    "data_full = []\n",
    "for q_idx, ele in enumerate(ds['train']):\n",
    "    if q_idx == 2538:\n",
    "        continue\n",
    "    input_ele = {}\n",
    "    input_ele['question_id'] = q_idx\n",
    "    input_ele['question'] = ele['question']\n",
    "    choices = ast.literal_eval(ele['options'])\n",
    "    if 'DK/Refused' in choices:\n",
    "        choices.remove('DK/Refused')\n",
    "    input_ele['options'] = choices\n",
    "\n",
    "    data_full.append(input_ele)\n",
    "\n",
    "with open('../data/GlobalOpinionQA/results/full.pkl', 'wb') as f:\n",
    "    pickle.dump(data_full, f)\n",
    "\n",
    "print(\"total number of questions in GlobalOpinionQA:\", len(data_full))\n",
    "\n",
    "res_org = qa.qa_eval_org(data_full)\n",
    "\n",
    "with open('../data/GlobalOpinionQA/results/org.pkl', 'wb') as f:\n",
    "    pickle.dump(res_org, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing QA: 100%|██████████| 2555/2555 [00:41<00:00, 61.15it/s]\n",
      "Processing QA: 100%|██████████| 2555/2555 [00:41<00:00, 61.53it/s] \n",
      "Processing QA: 100%|██████████| 2555/2555 [00:40<00:00, 62.77it/s] \n",
      "Processing QA: 100%|██████████| 2555/2555 [00:42<00:00, 60.40it/s] \n"
     ]
    }
   ],
   "source": [
    "with open('../data/GlobalOpinionQA/results/org.pkl', 'rb') as f:\n",
    "    res_org = pickle.load(f)\n",
    "\n",
    "type_names = ['1st', '2nd', 'rnd', 'lst']\n",
    "\n",
    "for disagree_type in type_names:\n",
    "    results = qa.qa_eval_one(res_org, disagree_type)\n",
    "    with open(f'../data/GlobalOpinionQA/results/one_{disagree_type}.pkl', 'wb') as f:\n",
    "        pickle.dump(results, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing QA batch 1: 100%|██████████| 1000/1000 [00:20<00:00, 49.77it/s]\n",
      "Processing QA batch 2: 100%|██████████| 1000/1000 [00:21<00:00, 46.04it/s]\n",
      "Processing QA batch 3: 100%|██████████| 1000/1000 [00:18<00:00, 53.86it/s]\n",
      "Processing QA batch 4: 100%|██████████| 1000/1000 [00:19<00:00, 50.88it/s]\n",
      "Processing QA batch 5: 100%|██████████| 1000/1000 [00:24<00:00, 40.80it/s]\n",
      "Processing QA batch 6: 100%|██████████| 110/110 [00:03<00:00, 34.01it/s]\n"
     ]
    }
   ],
   "source": [
    "with open('../data/GlobalOpinionQA/results/org.pkl', 'rb') as f:\n",
    "    res_org = pickle.load(f)\n",
    "\n",
    "results = qa.qa_eval_hierarchy(res_org, '2nd')\n",
    "\n",
    "with open(f'../data/GlobalOpinionQA/results/hierarchy_2nd.pkl', 'wb') as f:\n",
    "    pickle.dump(results, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MMLU-Pro: 12032, 9.47\n",
      "GPQA-Diamond: 198, 4.00\n",
      "ARC: 1172, 4.00\n",
      "OpinionQA: 1506, 3.24\n",
      "GlobalOpinionQA: 2555, 4.09\n",
      "SIQA: 1954, 3.00\n"
     ]
    }
   ],
   "source": [
    "dict_results = {}\n",
    "for benchmark in [\"MMLU-Pro\", \"GPQA-Diamond\", \"ARC\", \"OpinionQA\", \"GlobalOpinionQA\", \"SIQA\"]:\n",
    "    PATH = f'../data/{benchmark}/results/org.pkl'\n",
    "    with open(PATH, 'rb') as f:\n",
    "        retrieved_results = pickle.load(f)\n",
    "    list_len_options = [len(ele['options']) for ele in retrieved_results]\n",
    "    print(f\"{benchmark}: {len(retrieved_results)}, {np.mean(list_len_options) :.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C1: Origin of Herd Behavior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table 1 - dyad experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### how peer response is selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_results = {}\n",
    "dict_results['Average'] = {'1st':[], '2nd':[], 'rnd':[], 'lst':[]}\n",
    "for benchmark in [\"MMLU-Pro\", \"GPQA-Diamond\", \"ARC\", \"OpinionQA\", \"GlobalOpinionQA\", \"SIQA\"]:\n",
    "    dict_results[benchmark] = {}\n",
    "    for disagree_type in ['1st', '2nd', 'rnd', 'lst']:\n",
    "        PATH = f'../data/{benchmark}/results/one_{disagree_type}.pkl'\n",
    "        with open(PATH, 'rb') as f:\n",
    "            retrieved_results = pickle.load(f)\n",
    "            dict_results[benchmark][disagree_type] = retrieved_results\n",
    "            dict_results['Average'][disagree_type].extend(retrieved_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Method</th>\n",
       "      <th>MMLU-Pro</th>\n",
       "      <th>GPQA-Diamond</th>\n",
       "      <th>ARC</th>\n",
       "      <th>OpinionQA</th>\n",
       "      <th>GlobalOpinionQA</th>\n",
       "      <th>SIQA</th>\n",
       "      <th>Average</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1st</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2nd</td>\n",
       "      <td>0.51*</td>\n",
       "      <td>0.58*</td>\n",
       "      <td>0.09*</td>\n",
       "      <td>0.61*</td>\n",
       "      <td>0.69*</td>\n",
       "      <td>0.16*</td>\n",
       "      <td>0.48*</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>rnd</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>lst</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.29</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Method MMLU-Pro GPQA-Diamond    ARC OpinionQA GlobalOpinionQA   SIQA Average\n",
       "0    1st     0.03         0.05   0.01      0.01            0.01   0.02    0.03\n",
       "1    2nd    0.51*        0.58*  0.09*     0.61*           0.69*  0.16*   0.48*\n",
       "2    rnd     0.31          0.4   0.04      0.55             0.6   0.09    0.33\n",
       "3    lst     0.25         0.37   0.04      0.52            0.56   0.09    0.29"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "disagree_types = ['1st', '2nd', 'rnd', 'lst']\n",
    "\n",
    "table1 = pd.DataFrame(columns = [\"Method\", \"MMLU-Pro\", \"GPQA-Diamond\", \"ARC\", \"OpinionQA\", \"GlobalOpinionQA\", \"SIQA\", \"Average\"])\n",
    "# table1.loc[0] = [\"1st\", 0.75, 0.68, 0.66, 0.65, 0.63, 0.62]\n",
    "for disagree_type in ['1st', '2nd', 'rnd', 'lst']:\n",
    "    row = [disagree_type]\n",
    "    for benchmark in [\"MMLU-Pro\", \"GPQA-Diamond\", \"ARC\", \"OpinionQA\", \"GlobalOpinionQA\", \"SIQA\", \"Average\"]:\n",
    "        flip_rate = np.mean([r['r'] != r['r^org'] for r in dict_results[benchmark][disagree_type]])\n",
    "        row.append(round(flip_rate, 2))\n",
    "    table1.loc[len(table1)] = row\n",
    "\n",
    "# Apply significance marking\n",
    "columns_to_check = table1.columns[1:]  # include all benchmarks + 'Average'\n",
    "row_2nd = table1[table1['Method'] == '2nd'].iloc[0]\n",
    "\n",
    "for col in columns_to_check:\n",
    "    values = table1[col]\n",
    "    sorted_vals = sorted(values)\n",
    "    second_min = sorted_vals[1] if values[table1['Method'] == '2nd'].values[0] == sorted_vals[0] else sorted_vals[0]\n",
    "    second_min_method = values[values == second_min].index[0]\n",
    "    second_min_method = disagree_types[second_min_method]\n",
    "    \n",
    "    # Get per-sample flip_rate lists\n",
    "    vals_2nd = [r['r'] != r['r^org'] for r in dict_results[col]['2nd']]\n",
    "    vals_comp = [r['r'] != r['r^org'] for r in dict_results[col][second_min_method]]\n",
    "\n",
    "    # Run paired t-test\n",
    "    t_stat, p_value = ttest_rel(np.array(vals_2nd, dtype=float), np.array(vals_comp, dtype=float))\n",
    "\n",
    "    if p_value < 0.05:\n",
    "        table1 = table1.astype({col: 'object'}) # ensure column can hold strings\n",
    "        table1.loc[table1['Method'] == '2nd', col] = f\"{row_2nd[col]:.2f}*\"\n",
    "\n",
    "\n",
    "table1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Peer's Competence (Education)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing QA: 100%|██████████| 594/594 [00:12<00:00, 47.84it/s]\n",
      "Processing QA: 100%|██████████| 3516/3516 [00:58<00:00, 60.12it/s] \n",
      "Processing QA: 100%|██████████| 4518/4518 [01:08<00:00, 65.99it/s] \n",
      "Processing QA: 100%|██████████| 7665/7665 [01:58<00:00, 64.46it/s] \n",
      "Processing QA: 100%|██████████| 5862/5862 [01:36<00:00, 61.06it/s] \n"
     ]
    }
   ],
   "source": [
    "# benchmarks = [\"MMLU-Pro\", \"GPQA-Diamond\", \"ARC\", \"OpinionQA\", \"GlobalOpinionQA\", \"SIQA\"]\n",
    "\n",
    "\n",
    "# edu_results = {}\n",
    "# for benchmark in benchmarks:\n",
    "#     if benchmark == \"MMLU-Pro\":\n",
    "#         continue\n",
    "#     with open(f'../data/{benchmark}/results/org.pkl', 'rb') as f:\n",
    "#         res_org = pickle.load(f)\n",
    "\n",
    "#     results = qa.qa_eval_education(res_org, '2nd')\n",
    "#     edu_results[benchmark] = results\n",
    "\n",
    "#     with open(f'../data/{benchmark}/results/edu_2nd.pkl', 'wb') as f:\n",
    "#         pickle.dump(results, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Method</th>\n",
       "      <th>MMLU-Pro</th>\n",
       "      <th>GPQA-Diamond</th>\n",
       "      <th>ARC</th>\n",
       "      <th>OpinionQA</th>\n",
       "      <th>GlobalOpinionQA</th>\n",
       "      <th>SIQA</th>\n",
       "      <th>Average</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>graduate degree</td>\n",
       "      <td>0.5047*</td>\n",
       "      <td>0.5556*</td>\n",
       "      <td>0.0828</td>\n",
       "      <td>0.7583*</td>\n",
       "      <td>0.8282*</td>\n",
       "      <td>0.1464*</td>\n",
       "      <td>0.5059*</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>college degree</td>\n",
       "      <td>0.4732</td>\n",
       "      <td>0.4949</td>\n",
       "      <td>0.0785</td>\n",
       "      <td>0.7357</td>\n",
       "      <td>0.8305</td>\n",
       "      <td>0.1372</td>\n",
       "      <td>0.4831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>high school diploma</td>\n",
       "      <td>0.4427</td>\n",
       "      <td>0.4798</td>\n",
       "      <td>0.0742</td>\n",
       "      <td>0.7145</td>\n",
       "      <td>0.7945</td>\n",
       "      <td>0.1372</td>\n",
       "      <td>0.4575</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Method MMLU-Pro GPQA-Diamond     ARC OpinionQA  \\\n",
       "0      graduate degree  0.5047*      0.5556*  0.0828   0.7583*   \n",
       "1       college degree   0.4732       0.4949  0.0785    0.7357   \n",
       "2  high school diploma   0.4427       0.4798  0.0742    0.7145   \n",
       "\n",
       "  GlobalOpinionQA     SIQA  Average  \n",
       "0         0.8282*  0.1464*  0.5059*  \n",
       "1          0.8305   0.1372   0.4831  \n",
       "2          0.7945   0.1372   0.4575  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "benchmarks = [\"MMLU-Pro\", \"GPQA-Diamond\", \"ARC\", \"OpinionQA\", \"GlobalOpinionQA\", \"SIQA\"]\n",
    "table_edu = pd.DataFrame(columns = [\"Method\", \"MMLU-Pro\", \"GPQA-Diamond\", \"ARC\", \"OpinionQA\", \"GlobalOpinionQA\", \"SIQA\", \"Average\"])\n",
    "edu_types = ['graduate degree', 'college degree', 'high school diploma']\n",
    "\n",
    "edu_results = {}\n",
    "edu_results['Average'] = {e: [] for e in edu_types}\n",
    "for benchmark in benchmarks:\n",
    "    with open(f'../data/{benchmark}/results/edu_2nd.pkl', 'rb') as f:\n",
    "        retrieved_results = pickle.load(f)\n",
    "        edu_results[benchmark] = retrieved_results\n",
    "        edu_results['Average']['graduate degree'].extend(retrieved_results['graduate degree'])\n",
    "        edu_results['Average']['college degree'].extend(retrieved_results['college degree'])\n",
    "        edu_results['Average']['high school diploma'].extend(retrieved_results['high school diploma'])\n",
    "        \n",
    "\n",
    "for education_degree in ['graduate degree', 'college degree', 'high school diploma']:\n",
    "    row = [education_degree]\n",
    "    for benchmark in [\"MMLU-Pro\", \"GPQA-Diamond\", \"ARC\", \"OpinionQA\", \"GlobalOpinionQA\", \"SIQA\", \"Average\"]:\n",
    "        flip_rate = np.mean([r['r'] != r['r^org'] for r in edu_results[benchmark][education_degree]])\n",
    "        row.append(round(flip_rate, 4))\n",
    "    table_edu.loc[len(table_edu)] = row\n",
    "\n",
    "\n",
    "# Apply significance marking\n",
    "columns_to_check = table_edu.columns[1:]  # include all benchmarks + 'Average'\n",
    "row_graduate = table_edu[table_edu['Method'] == 'graduate degree'].iloc[0]\n",
    "\n",
    "for col in columns_to_check:\n",
    "    values = table_edu[col]\n",
    "    sorted_vals = sorted(values)\n",
    "    second_min = sorted_vals[1] if values[table_edu['Method'] == 'graduate degree'].values[0] == sorted_vals[0] else sorted_vals[0]\n",
    "    second_min_method = values[values == second_min].index[0]\n",
    "    second_min_method = edu_types[second_min_method]\n",
    "    \n",
    "    # Get per-sample flip_rate lists\n",
    "    vals_2nd = [r['r'] != r['r^org'] for r in edu_results[col]['graduate degree']]\n",
    "    vals_comp = [r['r'] != r['r^org'] for r in edu_results[col][second_min_method]]\n",
    "\n",
    "    # Run paired t-test\n",
    "    t_stat, p_value = ttest_rel(np.array(vals_2nd, dtype=float), np.array(vals_comp, dtype=float))\n",
    "\n",
    "    if p_value < 0.05:\n",
    "        table_edu = table_edu.astype({col: 'object'}) # ensure column can hold strings\n",
    "        table_edu.loc[table_edu['Method'] == 'graduate degree', col] = f\"{row_graduate[col]:.4f}*\"\n",
    "\n",
    "table_edu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In/Out domain Expertise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "category: business\n",
      "category: Physics\n",
      "no category\n",
      "no category\n",
      "no category\n",
      "no category\n"
     ]
    }
   ],
   "source": [
    "benchmarks = [\"MMLU-Pro\", \"GPQA-Diamond\", \"ARC\", \"OpinionQA\", \"GlobalOpinionQA\", \"SIQA\"]\n",
    "for benchmark in benchmarks:\n",
    "    with open(f'../data/{benchmark}/results/org.pkl', 'rb') as f:\n",
    "        mmlu_org = pickle.load(f)\n",
    "        if 'category' in mmlu_org[0].keys():\n",
    "            print(f'category: {mmlu_org[0][\"category\"]}')\n",
    "        else:\n",
    "            print('no category')\n",
    "\n",
    "# results = qa.qa_eval_domain(res_mmlu_org, '2nd')\n",
    "\n",
    "# with open(f'../data/MMLU-Pro/results/mmlu_domain_2nd.pkl', 'wb') as f:\n",
    "#     pickle.dump(results, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Method</th>\n",
       "      <th>MMLU-Pro</th>\n",
       "      <th>GPQA-Diamond</th>\n",
       "      <th>Average</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>in_domain</td>\n",
       "      <td>0.55*</td>\n",
       "      <td>0.72*</td>\n",
       "      <td>0.55*</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>out_domain</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.48</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Method MMLU-Pro GPQA-Diamond Average\n",
       "0   in_domain    0.55*        0.72*   0.55*\n",
       "1  out_domain     0.48         0.46    0.48"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "benchmarks = [\"MMLU-Pro\", \"GPQA-Diamond\"]\n",
    "table_domain = pd.DataFrame(columns = [\"Method\", \"MMLU-Pro\", \"GPQA-Diamond\",\"Average\"])\n",
    "list_domain = ['in_domain', 'out_domain']\n",
    "\n",
    "domain_results = {}\n",
    "domain_results['Average'] = {e: [] for e in list_domain}\n",
    "for benchmark in benchmarks:\n",
    "    with open(f'../data/{benchmark}/results/domain_2nd.pkl', 'rb') as f:\n",
    "        retrieved_results = pickle.load(f)\n",
    "        domain_results[benchmark] = retrieved_results\n",
    "        domain_results['Average']['in_domain'].extend(retrieved_results['in_domain'])\n",
    "        domain_results['Average']['out_domain'].extend(retrieved_results['out_domain'])\n",
    "        \n",
    "\n",
    "for domain in list_domain:\n",
    "    row = [domain]\n",
    "    for benchmark in [\"MMLU-Pro\", \"GPQA-Diamond\", \"Average\"]:\n",
    "        flip_rate = np.mean([r['r'] != r['r^org'] for r in domain_results[benchmark][domain]])\n",
    "        row.append(round(flip_rate, 2))\n",
    "    table_domain.loc[len(table_domain)] = row\n",
    "\n",
    "\n",
    "# Apply significance marking\n",
    "columns_to_check = table_domain.columns[1:]  # include all benchmarks + 'Average'\n",
    "row_in_domain = table_domain[table_domain['Method'] == 'in_domain'].iloc[0]\n",
    "\n",
    "for col in columns_to_check:\n",
    "    values = table_domain[col]\n",
    "    sorted_vals = sorted(values)\n",
    "    second_min = sorted_vals[1] if values[table_domain['Method'] == 'in_domain'].values[0] == sorted_vals[0] else sorted_vals[0]\n",
    "    second_min_method = values[values == second_min].index[0]\n",
    "    second_min_method = list_domain[second_min_method]\n",
    "    \n",
    "    # Get per-sample flip_rate lists\n",
    "    vals_2nd = [r['r'] != r['r^org'] for r in domain_results[col]['in_domain']]\n",
    "    vals_comp = [r['r'] != r['r^org'] for r in domain_results[col][second_min_method]]\n",
    "\n",
    "    # Run paired t-test\n",
    "    t_stat, p_value = ttest_rel(np.array(vals_2nd, dtype=float), np.array(vals_comp, dtype=float))\n",
    "\n",
    "    if p_value < 0.05:\n",
    "        table_domain = table_domain.astype({col: 'object'}) # ensure column can hold strings\n",
    "        table_domain.loc[table_domain['Method'] == 'in_domain', col] = f\"{row_in_domain[col]:.2f}*\"\n",
    "\n",
    "table_domain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Plotting consistency by education level\n",
    "\n",
    "# results_graduate = results['graduate degree']\n",
    "# results_college = results['college degree']\n",
    "# results_high_school = results['high school diploma']\n",
    "# results_control = res_mmlu_one\n",
    "\n",
    "# consistence_graduate  = np.array([r['r'] == r['r^org'] for r in results_graduate])\n",
    "# consistence_college   = np.array([r['r'] == r['r^org'] for r in results_college])\n",
    "# consistence_high_school = np.array([r['r'] == r['r^org'] for r in results_high_school])\n",
    "# consistence_control = np.array([r['r'] == r['r^org'] for r in results_control])\n",
    "\n",
    "# # Calculate means and confidence intervals\n",
    "# education_groups = [consistence_graduate, consistence_college, consistence_high_school, consistence_control]\n",
    "# mean_consistance = [group.mean() for group in education_groups]\n",
    "# confidence_intervals = [stats.t.interval(\n",
    "#     0.95, len(group)-1, loc=group.mean(), scale=stats.sem(group)) for group in education_groups]\n",
    "\n",
    "# # Extracting lower and upper bounds for plotting\n",
    "# ci_lower = [mean - ci[0] for mean, ci in zip(mean_consistance, confidence_intervals)]\n",
    "# ci_upper = [ci[1] - mean for mean, ci in zip(mean_consistance, confidence_intervals)]\n",
    "\n",
    "# education_levels = ['Graduate\\nDegree', 'College\\nDegree', 'High School\\nDiploma', 'Control']\n",
    "\n",
    "# # Plotting with confidence intervals\n",
    "# plt.figure(figsize=(4, 6))\n",
    "# plt.bar(education_levels, mean_consistance, yerr=[ci_lower, ci_upper], capsize=10,\n",
    "#         color=['skyblue', 'lightgreen', 'salmon', 'lightgray'], alpha=0.7)\n",
    "# plt.ylabel('Consistency (0-1)')\n",
    "# plt.xlabel('Education')\n",
    "# plt.title('Consistency by Education Level \\nwith 95% Confidence Intervals')\n",
    "# plt.ylim(0, 1)\n",
    "\n",
    "# # Display values on the bars\n",
    "# for i, val in enumerate(mean_consistance):\n",
    "#     plt.text(i, val + max(ci_upper)*1, f'{val:.2f}', ha='center', va='bottom')\n",
    "\n",
    "# plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Social Hierarchy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Method</th>\n",
       "      <th>MMLU-Pro</th>\n",
       "      <th>GPQA-Diamond</th>\n",
       "      <th>ARC</th>\n",
       "      <th>OpinionQA</th>\n",
       "      <th>GlobalOpinionQA</th>\n",
       "      <th>SIQA</th>\n",
       "      <th>Average</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>employer</td>\n",
       "      <td>0.5676*</td>\n",
       "      <td>0.7121</td>\n",
       "      <td>0.1024</td>\n",
       "      <td>0.7145*</td>\n",
       "      <td>0.7746*</td>\n",
       "      <td>0.2006*</td>\n",
       "      <td>0.5427*</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>employee</td>\n",
       "      <td>0.5331</td>\n",
       "      <td>0.7121</td>\n",
       "      <td>0.0947</td>\n",
       "      <td>0.7424</td>\n",
       "      <td>0.7855</td>\n",
       "      <td>0.174</td>\n",
       "      <td>0.5218</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Method MMLU-Pro  GPQA-Diamond     ARC OpinionQA GlobalOpinionQA     SIQA  \\\n",
       "0  employer  0.5676*        0.7121  0.1024   0.7145*         0.7746*  0.2006*   \n",
       "1  employee   0.5331        0.7121  0.0947    0.7424          0.7855    0.174   \n",
       "\n",
       "   Average  \n",
       "0  0.5427*  \n",
       "1   0.5218  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "benchmarks = [\"MMLU-Pro\", \"GPQA-Diamond\", \"ARC\", \"OpinionQA\", \"GlobalOpinionQA\", \"SIQA\"]\n",
    "table_domain = pd.DataFrame(columns = [\"Method\"]+benchmarks+[\"Average\"])\n",
    "list_domain = ['employer', 'employee']\n",
    "\n",
    "domain_results = {}\n",
    "domain_results['Average'] = {e: [] for e in list_domain}\n",
    "for benchmark in benchmarks:\n",
    "    with open(f'../data/{benchmark}/results/hierarchy_2nd.pkl', 'rb') as f:\n",
    "        retrieved_results = pickle.load(f)\n",
    "        domain_results[benchmark] = retrieved_results\n",
    "        domain_results['Average']['employer'].extend(retrieved_results['employer'])\n",
    "        domain_results['Average']['employee'].extend(retrieved_results['employee'])\n",
    "        \n",
    "\n",
    "for domain in list_domain:\n",
    "    row = [domain]\n",
    "    for benchmark in benchmarks+[\"Average\"]:\n",
    "        flip_rate = np.mean([r['r'] != r['r^org'] for r in domain_results[benchmark][domain]])\n",
    "        row.append(round(flip_rate, 4))\n",
    "    table_domain.loc[len(table_domain)] = row\n",
    "\n",
    "\n",
    "# Apply significance marking\n",
    "columns_to_check = table_domain.columns[1:]  # include all benchmarks + 'Average'\n",
    "row_in_domain = table_domain[table_domain['Method'] == 'employer'].iloc[0]\n",
    "\n",
    "for col in columns_to_check:\n",
    "    values = table_domain[col]\n",
    "    sorted_vals = sorted(values)\n",
    "    second_min = sorted_vals[1] if values[table_domain['Method'] == 'employer'].values[0] == sorted_vals[0] else sorted_vals[0]\n",
    "    second_min_method = values[values == second_min].index[0]\n",
    "    second_min_method = list_domain[second_min_method]\n",
    "    \n",
    "    # Get per-sample flip_rate lists\n",
    "    vals_2nd = [r['r'] != r['r^org'] for r in domain_results[col]['employer']]\n",
    "    vals_comp = [r['r'] != r['r^org'] for r in domain_results[col][second_min_method]]\n",
    "\n",
    "    # Run paired t-test\n",
    "    t_stat, p_value = ttest_rel(np.array(vals_2nd, dtype=float), np.array(vals_comp, dtype=float))\n",
    "\n",
    "    if p_value < 0.05:\n",
    "        table_domain = table_domain.astype({col: 'object'}) # ensure column can hold strings\n",
    "        table_domain.loc[table_domain['Method'] == 'employer', col] = f\"{row_in_domain[col]:.4f}*\"\n",
    "\n",
    "table_domain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table 1.5 flip rate and accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_results = {}\n",
    "dict_results['Average'] = {'org':[], '1st':[], '2nd':[], 'rnd':[], 'lst':[]}\n",
    "benchmarks = [\"MMLU-Pro\", \"GPQA-Diamond\", \"ARC\"]\n",
    "for benchmark in benchmarks:\n",
    "    dict_results[benchmark] = {}\n",
    "    with open(f'../data/{benchmark}/results/org.pkl', 'rb') as f:\n",
    "        retrieved_results = pickle.load(f)\n",
    "        dict_results[benchmark]['org'] = retrieved_results\n",
    "        dict_results['Average']['org'].extend(retrieved_results)\n",
    "    for disagree_type in ['1st', '2nd', 'rnd', 'lst']:\n",
    "        PATH = f'../data/{benchmark}/results/one_{disagree_type}.pkl'\n",
    "        with open(PATH, 'rb') as f:\n",
    "            retrieved_results = pickle.load(f)\n",
    "            dict_results[benchmark][disagree_type] = retrieved_results\n",
    "            dict_results['Average'][disagree_type].extend(retrieved_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Method</th>\n",
       "      <th>MMLU-Pro</th>\n",
       "      <th>GPQA-Diamond</th>\n",
       "      <th>ARC</th>\n",
       "      <th>Average</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>org</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1st</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2nd</td>\n",
       "      <td>0.41*</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.90*</td>\n",
       "      <td>0.45*</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>rnd</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.91</td>\n",
       "      <td>0.46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>lst</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.91</td>\n",
       "      <td>0.48</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Method MMLU-Pro  GPQA-Diamond    ARC Average\n",
       "0    org     0.45          0.35   0.93    0.49\n",
       "1    1st     0.45          0.34   0.92    0.49\n",
       "2    2nd    0.41*          0.30  0.90*   0.45*\n",
       "3    rnd     0.42          0.32   0.91    0.46\n",
       "4    lst     0.43          0.35   0.91    0.48"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.stats import ttest_rel\n",
    "\n",
    "disagree_types = ['org', '1st', '2nd', 'rnd', 'lst']\n",
    "\n",
    "table1 = pd.DataFrame(columns = [\"Method\"] + benchmarks +[\"Average\"])\n",
    "# table1.loc[0] = [\"1st\", 0.75, 0.68, 0.66, 0.65, 0.63, 0.62]\n",
    "for disagree_type in disagree_types:\n",
    "    row = [disagree_type]\n",
    "    for benchmark in benchmarks + [\"Average\"]:\n",
    "        if disagree_type == 'org':\n",
    "            accuracy = np.mean([r['r^org'] == r['answer'] for r in dict_results[benchmark][disagree_type]])\n",
    "        else:\n",
    "            accuracy = np.mean([r['r'] == r['answer'] for r in dict_results[benchmark][disagree_type]])\n",
    "        row.append(round(accuracy, 2))\n",
    "    table1.loc[len(table1)] = row\n",
    "\n",
    "# Apply significance marking\n",
    "columns_to_check = table1.columns[1:]  # include all benchmarks + 'Average'\n",
    "row_2nd = table1[table1['Method'] == '2nd'].iloc[0]\n",
    "\n",
    "for col in columns_to_check:\n",
    "    values = table1[col]\n",
    "    sorted_vals = sorted(values)\n",
    "    second_min = sorted_vals[1] if values[table1['Method'] == '2nd'].values[0] == sorted_vals[0] else sorted_vals[0]\n",
    "    second_min_method = values[values == second_min].index[0]\n",
    "    second_min_method = disagree_types[second_min_method]\n",
    "    \n",
    "    # Get per-sample accuracy lists\n",
    "    vals_2nd = [r['r'] == r['answer'] for r in dict_results[col]['2nd']]\n",
    "    vals_comp = [r['r'] == r['answer'] for r in dict_results[col][second_min_method]]\n",
    "\n",
    "    # Run paired t-test\n",
    "    t_stat, p_value = ttest_rel(np.array(vals_2nd, dtype=float), np.array(vals_comp, dtype=float))\n",
    "\n",
    "    if p_value < 0.05:\n",
    "        table1 = table1.astype({col: 'object'}) # ensure column can hold strings\n",
    "        table1.loc[table1['Method'] == '2nd', col] = f\"{row_2nd[col]:.2f}*\"\n",
    "\n",
    "\n",
    "table1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Method</th>\n",
       "      <th>MMLU-Pro</th>\n",
       "      <th>GPQA-Diamond</th>\n",
       "      <th>ARC</th>\n",
       "      <th>Average</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>graduate degree</td>\n",
       "      <td>0.40*</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.44*</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>college degree</td>\n",
       "      <td>0.41</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>high school diploma</td>\n",
       "      <td>0.41</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.45</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Method MMLU-Pro  GPQA-Diamond   ARC Average\n",
       "0      graduate degree    0.40*          0.29  0.89   0.44*\n",
       "1       college degree     0.41          0.32  0.90    0.45\n",
       "2  high school diploma     0.41          0.31  0.90    0.45"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table_edu = pd.DataFrame(columns = [\"Method\"] + benchmarks +[\"Average\"])\n",
    "edu_types = ['graduate degree', 'college degree', 'high school diploma']\n",
    "\n",
    "edu_results = {}\n",
    "edu_results['Average'] = {e: [] for e in edu_types}\n",
    "for benchmark in benchmarks:\n",
    "    with open(f'../data/{benchmark}/results/edu_2nd.pkl', 'rb') as f:\n",
    "        retrieved_results = pickle.load(f)\n",
    "        edu_results[benchmark] = retrieved_results\n",
    "        edu_results['Average']['graduate degree'].extend(retrieved_results['graduate degree'])\n",
    "        edu_results['Average']['college degree'].extend(retrieved_results['college degree'])\n",
    "        edu_results['Average']['high school diploma'].extend(retrieved_results['high school diploma'])\n",
    "        \n",
    "\n",
    "for education_degree in ['graduate degree', 'college degree', 'high school diploma']:\n",
    "    row = [education_degree]\n",
    "    for benchmark in benchmarks + [\"Average\"]:\n",
    "        accuracy = np.mean([r['r'] == r['answer'] for r in edu_results[benchmark][education_degree]])\n",
    "        row.append(round(accuracy, 2))\n",
    "    table_edu.loc[len(table_edu)] = row\n",
    "\n",
    "\n",
    "# Apply significance marking\n",
    "columns_to_check = table_edu.columns[1:]  # include all benchmarks + 'Average'\n",
    "row_graduate = table_edu[table_edu['Method'] == 'graduate degree'].iloc[0]\n",
    "\n",
    "for col in columns_to_check:\n",
    "    values = table_edu[col]\n",
    "    sorted_vals = sorted(values)\n",
    "    second_min = sorted_vals[1] if values[table_edu['Method'] == 'graduate degree'].values[0] == sorted_vals[0] else sorted_vals[0]\n",
    "    second_min_method = values[values == second_min].index[0]\n",
    "    second_min_method = edu_types[second_min_method]\n",
    "    \n",
    "    # Get per-sample accuracy lists\n",
    "    vals_2nd = [r['r'] == r['answer'] for r in edu_results[col]['graduate degree']]\n",
    "    vals_comp = [r['r'] == r['answer'] for r in edu_results[col][second_min_method]]\n",
    "\n",
    "    # Run paired t-test\n",
    "    t_stat, p_value = ttest_rel(np.array(vals_2nd, dtype=float), np.array(vals_comp, dtype=float))\n",
    "\n",
    "    if p_value < 0.05:\n",
    "        table_edu = table_edu.astype({col: 'object'}) # ensure column can hold strings\n",
    "        table_edu.loc[table_edu['Method'] == 'graduate degree', col] = f\"{row_graduate[col]:.2f}*\"\n",
    "\n",
    "table_edu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Method</th>\n",
       "      <th>MMLU-Pro</th>\n",
       "      <th>GPQA-Diamond</th>\n",
       "      <th>Average</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>in_domain</td>\n",
       "      <td>0.40*</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>out_domain</td>\n",
       "      <td>0.41</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Method MMLU-Pro  GPQA-Diamond  Average\n",
       "0   in_domain    0.40*          0.29      0.4\n",
       "1  out_domain     0.41          0.29      0.4"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "benchmarks = [\"MMLU-Pro\", \"GPQA-Diamond\"]\n",
    "table_domain = pd.DataFrame(columns = [\"Method\", \"MMLU-Pro\", \"GPQA-Diamond\",\"Average\"])\n",
    "list_domain = ['in_domain', 'out_domain']\n",
    "\n",
    "domain_results = {}\n",
    "domain_results['Average'] = {e: [] for e in list_domain}\n",
    "for benchmark in benchmarks:\n",
    "    with open(f'../data/{benchmark}/results/domain_2nd.pkl', 'rb') as f:\n",
    "        retrieved_results = pickle.load(f)\n",
    "        domain_results[benchmark] = retrieved_results\n",
    "        domain_results['Average']['in_domain'].extend(retrieved_results['in_domain'])\n",
    "        domain_results['Average']['out_domain'].extend(retrieved_results['out_domain'])\n",
    "        \n",
    "\n",
    "for domain in list_domain:\n",
    "    row = [domain]\n",
    "    for benchmark in [\"MMLU-Pro\", \"GPQA-Diamond\", \"Average\"]:\n",
    "        accuracy = np.mean([r['r'] == r['answer'] for r in domain_results[benchmark][domain]])\n",
    "        row.append(round(accuracy, 2))\n",
    "    table_domain.loc[len(table_domain)] = row\n",
    "\n",
    "\n",
    "# Apply significance marking\n",
    "columns_to_check = table_domain.columns[1:]  # include all benchmarks + 'Average'\n",
    "row_in_domain = table_domain[table_domain['Method'] == 'in_domain'].iloc[0]\n",
    "\n",
    "for col in columns_to_check:\n",
    "    values = table_domain[col]\n",
    "    sorted_vals = sorted(values)\n",
    "    second_min = sorted_vals[1] if values[table_domain['Method'] == 'in_domain'].values[0] == sorted_vals[0] else sorted_vals[0]\n",
    "    second_min_method = values[values == second_min].index[0]\n",
    "    second_min_method = list_domain[second_min_method]\n",
    "    \n",
    "    # Get per-sample accuracy lists\n",
    "    vals_2nd = [r['r'] == r['answer'] for r in domain_results[col]['in_domain']]\n",
    "    vals_comp = [r['r'] == r['answer'] for r in domain_results[col][second_min_method]]\n",
    "\n",
    "    # Run paired t-test\n",
    "    t_stat, p_value = ttest_rel(np.array(vals_2nd, dtype=float), np.array(vals_comp, dtype=float))\n",
    "\n",
    "    if p_value < 0.05:\n",
    "        table_domain = table_domain.astype({col: 'object'}) # ensure column can hold strings\n",
    "        table_domain.loc[table_domain['Method'] == 'in_domain', col] = f\"{row_in_domain[col]:.2f}*\"\n",
    "\n",
    "table_domain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Method</th>\n",
       "      <th>MMLU-Pro</th>\n",
       "      <th>GPQA-Diamond</th>\n",
       "      <th>SIQA</th>\n",
       "      <th>Average</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>employer</td>\n",
       "      <td>0.40*</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.74*</td>\n",
       "      <td>0.44*</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>employee</td>\n",
       "      <td>0.41</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.45</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Method MMLU-Pro  GPQA-Diamond   SIQA Average\n",
       "0  employer    0.40*          0.27  0.74*   0.44*\n",
       "1  employee     0.41          0.27   0.76    0.45"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "benchmarks = [\"MMLU-Pro\", \"GPQA-Diamond\", \"SIQA\"]\n",
    "table_domain = pd.DataFrame(columns = [\"Method\"]+benchmarks+[\"Average\"])\n",
    "list_domain = ['employer', 'employee']\n",
    "\n",
    "domain_results = {}\n",
    "domain_results['Average'] = {e: [] for e in list_domain}\n",
    "for benchmark in benchmarks:\n",
    "    with open(f'../data/{benchmark}/results/hierarchy_2nd.pkl', 'rb') as f:\n",
    "        retrieved_results = pickle.load(f)\n",
    "        domain_results[benchmark] = retrieved_results\n",
    "        domain_results['Average']['employer'].extend(retrieved_results['employer'])\n",
    "        domain_results['Average']['employee'].extend(retrieved_results['employee'])\n",
    "        \n",
    "\n",
    "for domain in list_domain:\n",
    "    row = [domain]\n",
    "    for benchmark in benchmarks+[\"Average\"]:\n",
    "        accuracy = np.mean([r['r'] == r['answer'] for r in domain_results[benchmark][domain]])\n",
    "        row.append(round(accuracy, 2))\n",
    "    table_domain.loc[len(table_domain)] = row\n",
    "\n",
    "\n",
    "# Apply significance marking\n",
    "columns_to_check = table_domain.columns[1:]  # include all benchmarks + 'Average'\n",
    "row_in_domain = table_domain[table_domain['Method'] == 'employer'].iloc[0]\n",
    "\n",
    "for col in columns_to_check:\n",
    "    values = table_domain[col]\n",
    "    sorted_vals = sorted(values)\n",
    "    second_min = sorted_vals[1] if values[table_domain['Method'] == 'employer'].values[0] == sorted_vals[0] else sorted_vals[0]\n",
    "    second_min_method = values[values == second_min].index[0]\n",
    "    second_min_method = list_domain[second_min_method]\n",
    "    \n",
    "    # Get per-sample accuracy lists\n",
    "    vals_2nd = [r['r'] == r['answer'] for r in domain_results[col]['employer']]\n",
    "    vals_comp = [r['r'] == r['answer'] for r in domain_results[col][second_min_method]]\n",
    "\n",
    "    # Run paired t-test\n",
    "    t_stat, p_value = ttest_rel(np.array(vals_2nd, dtype=float), np.array(vals_comp, dtype=float))\n",
    "\n",
    "    if p_value < 0.05:\n",
    "        table_domain = table_domain.astype({col: 'object'}) # ensure column can hold strings\n",
    "        table_domain.loc[table_domain['Method'] == 'employer', col] = f\"{row_in_domain[col]:.2f}*\"\n",
    "\n",
    "table_domain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### self/peer confidence heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "objective_results = []\n",
    "subjective_results = []\n",
    "for benchmark in [\"OpinionQA\", \"SIQA\", \"GlobalOpinionQA\"]:\n",
    "    for disagree_type in ['2nd', 'rnd', 'lst']:\n",
    "        PATH = f'../data/{benchmark}/results/one_{disagree_type}.pkl'\n",
    "        with open(PATH, 'rb') as f:\n",
    "            subjective_results.extend(pickle.load(f))\n",
    "\n",
    "for benchmark in [\"MMLU-Pro\", \"GPQA-Diamond\", \"ARC\"]:\n",
    "    for disagree_type in ['2nd', 'rnd', 'lst']:\n",
    "        PATH = f'../data/{benchmark}/results/one_{disagree_type}.pkl'\n",
    "        with open(PATH, 'rb') as f:\n",
    "            objective_results.extend(pickle.load(f))\n",
    "\n",
    "results = objective_results + subjective_results\n",
    "\n",
    "# Extract accuracy and confidence\n",
    "# accuracy = np.array([r['r'] == r['answer'] for r in results])\n",
    "flip_rate = np.array([r['r'] != r['r^org'] for r in results])\n",
    "self_conf = np.array([r['p_r^org'] for r in results])\n",
    "other_conf = np.array([r['p_r_j'] for r in results])\n",
    "\n",
    "# Bin confidence scores into 10 bins from 0 to 1\n",
    "bins = np.linspace(0, 1, 11)\n",
    "row_bin_indices = np.digitize(other_conf, bins) - 1  # Adjust bin index to start at 0\n",
    "col_bin_indices = np.digitize(self_conf, bins) - 1\n",
    "\n",
    "heatmap_matrix = np.zeros((10, 10))\n",
    "for i in range(10):\n",
    "    for j in range(10):\n",
    "        indices = (row_bin_indices == i) & (col_bin_indices == j)\n",
    "        if np.any(indices):\n",
    "            avg_acc = flip_rate[indices].mean()\n",
    "        else:\n",
    "            avg_acc = np.nan\n",
    "        heatmap_matrix[i, j] = avg_acc\n",
    "\n",
    "row_labels = ['{:.2f}'.format(i/10) for i in range(10)]\n",
    "col_labels = ['{:.2f}'.format(i/10) for i in range(10)]\n",
    "x_label = 'Self-Confidence'\n",
    "y_label = 'Perceived Confidence'\n",
    "# title = 'Flip Rate vs Self/Peer Confidence'\n",
    "vlimit = (0, 1)\n",
    "df = pd.DataFrame(heatmap_matrix, index=row_labels, columns=col_labels)\n",
    "df = df.iloc[::-1]\n",
    "df = df.dropna(how='all')\n",
    "\n",
    "plt.figure(figsize=(8, 4), dpi=300)\n",
    "\n",
    "if vlimit:\n",
    "    vmin, vmax = vlimit\n",
    "    sns.heatmap(df, annot=True, cmap=\"coolwarm\", fmt=\".2f\", linewidths=0.5, vmin=vmin, vmax=vmax)\n",
    "else:\n",
    "    sns.heatmap(df, annot=True, cmap=\"coolwarm\", fmt=\".2f\", linewidths=0.5)\n",
    "\n",
    "plt.xlabel(x_label, fontsize=16)\n",
    "plt.ylabel(y_label, fontsize=16)\n",
    "plt.xticks(fontsize=12)\n",
    "plt.yticks(fontsize=12)\n",
    "# plt.title(title)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### diff in self/peer confidence or objective/subjective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_objective = pd.DataFrame()\n",
    "df_objective[\"p_r^org\"] = [r['p_r^org'] for r in objective_results]\n",
    "df_objective[\"p_r_j\"] = [r['p_r_j'] for r in objective_results]\n",
    "df_objective[\"change\"] = [r['r'] == r['r_j'] for r in objective_results]\n",
    "df_objective[\"task\"] = \"objective\"\n",
    "\n",
    "df_subjective = pd.DataFrame()\n",
    "df_subjective[\"p_r^org\"] = [r['p_r^org'] for r in subjective_results]\n",
    "df_subjective[\"p_r_j\"] = [r['p_r_j'] for r in subjective_results]\n",
    "df_subjective[\"change\"] = [r['r'] == r['r_j'] for r in subjective_results]\n",
    "df_subjective[\"task\"] = \"subjective\"\n",
    "\n",
    "df_all = pd.concat([df_objective, df_subjective], ignore_index=True)\n",
    "df_all[\"diff_p\"] = df_all[\"p_r^org\"] - df_all[\"p_r_j\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.466880\n",
      "         Iterations 6\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                 change   No. Observations:                58251\n",
      "Model:                          Logit   Df Residuals:                    58247\n",
      "Method:                           MLE   Df Model:                            3\n",
      "Date:                Sat, 17 May 2025   Pseudo R-squ.:                  0.2072\n",
      "Time:                        03:11:51   Log-Likelihood:                -27196.\n",
      "converged:                       True   LL-Null:                       -34306.\n",
      "Covariance Type:            nonrobust   LLR p-value:                     0.000\n",
      "=============================================================================================\n",
      "                                coef    std err          z      P>|z|      [0.025      0.975]\n",
      "---------------------------------------------------------------------------------------------\n",
      "Intercept                     1.5433      0.033     46.597      0.000       1.478       1.608\n",
      "task[T.subjective]            3.3019      0.128     25.781      0.000       3.051       3.553\n",
      "diff_p                       -4.0058      0.047    -85.739      0.000      -4.097      -3.914\n",
      "task[T.subjective]:diff_p    -1.7649      0.139    -12.669      0.000      -2.038      -1.492\n",
      "=============================================================================================\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "# Convert 'change' to integer (0/1)\n",
    "df_all['change'] = df_all['change'].astype(int)\n",
    "\n",
    "# Ensure 'task' is a categorical variable\n",
    "df_all['task'] = df_all['task'].astype('category')\n",
    "\n",
    "# Fit logistic regression model\n",
    "model = smf.logit(formula='change ~ diff_p + task + task * diff_p', data=df_all).fit()\n",
    "\n",
    "# Show summary\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Prepare prediction range\n",
    "diff_p_range = np.linspace(df_all[\"diff_p\"].min(), df_all[\"diff_p\"].max(), 100)\n",
    "\n",
    "# Create a dataframe for predictions\n",
    "predict_df = pd.DataFrame({\n",
    "    \"diff_p\": np.tile(diff_p_range, 2),\n",
    "    \"task\": np.repeat([\"objective\", \"subjective\"], len(diff_p_range))\n",
    "})\n",
    "\n",
    "# Predict probabilities\n",
    "predict_df[\"change_prob\"] = model.predict(predict_df)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(8, 4))\n",
    "sns.lineplot(data=predict_df, x=\"diff_p\", y=\"change_prob\", hue=\"task\")\n",
    "plt.title(\"Flip Rate vs. Confidence Difference\")\n",
    "plt.xlabel(\"Confidence Difference (p_r^org - p_r_j)\")\n",
    "plt.ylabel(\"Flip Rate\")\n",
    "plt.axvline(0, color='gray', linestyle='--', linewidth=1)\n",
    "plt.legend(title=\"Task Type\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C2: Format of Peer Information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Sampling (200 cap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# benchmarks = [\"MMLU-Pro\", \"GPQA-Diamond\", \"ARC\", \"OpinionQA\", \"GlobalOpinionQA\", \"SIQA\"]\n",
    "# for benchmark in benchmarks:\n",
    "#     with open(f'../data/{benchmark}/results/org.pkl', 'rb') as f:\n",
    "#         res_org = pickle.load(f)\n",
    "#     sample_size = min(200, len(res_org))\n",
    "#     sample_res_org = random.Random(benchmark).sample(res_org, sample_size)\n",
    "#     with open(f'../data/{benchmark}/sample_results/org.pkl', 'wb') as f:\n",
    "#         pickle.dump(sample_res_org, f)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmarks = [\"MMLU-Pro\", \"GPQA-Diamond\", \"ARC\", \"OpinionQA\", \"GlobalOpinionQA\", \"SIQA\"]\n",
    "factual = [\"MMLU-Pro\", \"GPQA-Diamond\", \"ARC\"]\n",
    "opinion = [\"OpinionQA\", \"GlobalOpinionQA\", \"SIQA\"]\n",
    "\n",
    "for benchmark in benchmarks:\n",
    "\n",
    "    with open(f'../data/{benchmark}/sample_results/org.pkl', 'rb') as f:\n",
    "        res_org = pickle.load(f)\n",
    "\n",
    "    input_feat_list = []\n",
    "    for disagree_size in range(1, 11): # row\n",
    "        input_row = []\n",
    "        for agree_size in range(1, 11): # column\n",
    "            \n",
    "            eval_feat = {\n",
    "                'type': 'grp_count',\n",
    "                'agree_size': agree_size,\n",
    "                'disagree_size': disagree_size,\n",
    "                'disagree_type': '2nd',\n",
    "                'q_type': 'factual' if benchmark in factual else 'opinion',\n",
    "                'order': 'random'\n",
    "            }\n",
    "            input_row.append(eval_feat)\n",
    "        input_feat_list.append(input_row)\n",
    "\n",
    "    results, accuracy = qa.qa_eval_matrix(res_org, input_feat_list)\n",
    "\n",
    "    with open(f'../data/{benchmark}/sample_results/grp_count_10.pkl', 'wb') as f:\n",
    "        pickle.dump(results, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmarks = [\"MMLU-Pro\", \"GPQA-Diamond\", \"ARC\", \"OpinionQA\", \"GlobalOpinionQA\", \"SIQA\"]\n",
    "factual = [\"MMLU-Pro\", \"GPQA-Diamond\", \"ARC\"]\n",
    "opinion = [\"OpinionQA\", \"GlobalOpinionQA\", \"SIQA\"]\n",
    "\n",
    "for benchmark in benchmarks:\n",
    "\n",
    "    with open(f'../data/{benchmark}/sample_results/org.pkl', 'rb') as f:\n",
    "        res_org = pickle.load(f)\n",
    "\n",
    "\n",
    "    input_feat_list = []\n",
    "    for disagree_size in range(1, 11): # row\n",
    "        input_row = []\n",
    "        for agree_size in range(1, 11): # column\n",
    "\n",
    "            eval_feat = {\n",
    "                'type': 'grp_ratio',\n",
    "                'agree_size': agree_size,\n",
    "                'disagree_size': disagree_size,\n",
    "                'disagree_type': '2nd',\n",
    "                'q_type': 'factual' if benchmark in factual else 'opinion',\n",
    "                'order': 'random'\n",
    "            }\n",
    "            input_row.append(eval_feat)\n",
    "        input_feat_list.append(input_row)\n",
    "\n",
    "    results, accuracy = qa.qa_eval_matrix(res_org, input_feat_list)\n",
    "\n",
    "    with open(f'../data/{benchmark}/sample_results/grp_ratio_10.pkl', 'wb') as f:\n",
    "        pickle.dump(results, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmarks = [\"MMLU-Pro\", \"GPQA-Diamond\", \"ARC\", \"OpinionQA\", \"GlobalOpinionQA\", \"SIQA\"]\n",
    "factual = [\"MMLU-Pro\", \"GPQA-Diamond\", \"ARC\"]\n",
    "opinion = [\"OpinionQA\", \"GlobalOpinionQA\", \"SIQA\"]\n",
    "\n",
    "for benchmark in benchmarks:\n",
    "\n",
    "    with open(f'../data/{benchmark}/sample_results/org.pkl', 'rb') as f:\n",
    "        res_org = pickle.load(f)\n",
    "\n",
    "\n",
    "    input_feat_list = []\n",
    "    for disagree_size in range(1, 11): # row\n",
    "        input_row = []\n",
    "        for agree_size in range(1, 11): # column\n",
    "            \n",
    "            eval_feat = {\n",
    "                'type': 'grp_list',\n",
    "                'agree_size': agree_size,\n",
    "                'disagree_size': disagree_size,\n",
    "                'disagree_type': '2nd',\n",
    "                'q_type': 'factual' if benchmark in factual else 'opinion',\n",
    "                'order': 'random'\n",
    "            }\n",
    "            input_row.append(eval_feat)\n",
    "        input_feat_list.append(input_row)\n",
    "\n",
    "    results, accuracy = qa.qa_eval_matrix(res_org, input_feat_list)\n",
    "\n",
    "    with open(f'../data/{benchmark}/sample_results/grp_list_10.pkl', 'wb') as f:\n",
    "        pickle.dump(results, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmarks = [\"MMLU-Pro\", \"GPQA-Diamond\", \"ARC\", \"OpinionQA\", \"GlobalOpinionQA\", \"SIQA\"]\n",
    "factual = [\"MMLU-Pro\", \"GPQA-Diamond\", \"ARC\"]\n",
    "opinion = [\"OpinionQA\", \"GlobalOpinionQA\", \"SIQA\"]\n",
    "\n",
    "for benchmark in benchmarks:\n",
    "\n",
    "    with open(f'../data/{benchmark}/sample_results/org.pkl', 'rb') as f:\n",
    "        res_org = pickle.load(f)\n",
    "\n",
    "    org_reason = qa.qa_generate_reason(res_org, '2nd', 10)\n",
    "\n",
    "    with open(f'../data/{benchmark}/sample_results/org_reason_10.pkl', 'wb') as f:\n",
    "        pickle.dump(org_reason, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmarks = [\"MMLU-Pro\", \"GPQA-Diamond\", \"ARC\", \"OpinionQA\", \"GlobalOpinionQA\", \"SIQA\"]\n",
    "factual = [\"MMLU-Pro\", \"GPQA-Diamond\", \"ARC\"]\n",
    "opinion = [\"OpinionQA\", \"GlobalOpinionQA\", \"SIQA\"]\n",
    "\n",
    "for benchmark in benchmarks:\n",
    "\n",
    "    with open(f'../data/{benchmark}/sample_results/org_reason_10.pkl', 'rb') as f:\n",
    "        org_reason = pickle.load(f)\n",
    "\n",
    "    input_feat_list = []\n",
    "    for disagree_size in range(1, 11): # row\n",
    "        input_row = []\n",
    "        for agree_size in range(1, 11): # column\n",
    "\n",
    "            eval_feat = {\n",
    "                'type': 'grp_disc',\n",
    "                'agree_size': agree_size,\n",
    "                'disagree_size': disagree_size,\n",
    "                'disagree_type': '2nd',\n",
    "                'q_type': 'factual' if benchmark in factual else 'opinion',\n",
    "                'order': 'random',\n",
    "                'use_reason': False\n",
    "            }\n",
    "            input_row.append(eval_feat)\n",
    "        input_feat_list.append(input_row)\n",
    "\n",
    "    results, accuracy = qa.qa_eval_matrix(org_reason, input_feat_list)\n",
    "\n",
    "    with open(f'../data/{benchmark}/sample_results/grp_disc_10.pkl', 'wb') as f:\n",
    "        pickle.dump(results, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmarks = [\"MMLU-Pro\", \"GPQA-Diamond\", \"ARC\", \"OpinionQA\", \"GlobalOpinionQA\", \"SIQA\"]\n",
    "factual = [\"MMLU-Pro\", \"GPQA-Diamond\", \"ARC\"]\n",
    "opinion = [\"OpinionQA\", \"GlobalOpinionQA\", \"SIQA\"]\n",
    "\n",
    "for benchmark in benchmarks:\n",
    "\n",
    "    with open(f'../data/{benchmark}/sample_results/org_reason_10.pkl', 'rb') as f:\n",
    "        org_reason = pickle.load(f)\n",
    "\n",
    "    input_feat_list = []\n",
    "    for disagree_size in range(1, 11): # row\n",
    "        input_row = []\n",
    "        for agree_size in range(1, 11): # column\n",
    "\n",
    "            eval_feat = {\n",
    "                'type': 'grp_disc',\n",
    "                'agree_size': agree_size,\n",
    "                'disagree_size': disagree_size,\n",
    "                'disagree_type': '2nd',\n",
    "                'q_type': 'factual' if benchmark in factual else 'opinion',\n",
    "                'order': 'random',\n",
    "                'use_reason': True\n",
    "            }\n",
    "            input_row.append(eval_feat)\n",
    "        input_feat_list.append(input_row)\n",
    "\n",
    "    results, accuracy = qa.qa_eval_matrix(org_reason, input_feat_list)\n",
    "\n",
    "    with open(f'../data/{benchmark}/sample_results/grp_reason_10.pkl', 'wb') as f:\n",
    "        pickle.dump(results, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table 3: Format of Peer Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten(nested_list):\n",
    "    flat = []\n",
    "    for item in nested_list:\n",
    "        if isinstance(item, list):\n",
    "            flat.extend(flatten(item))\n",
    "        else:\n",
    "            flat.append(item)\n",
    "    return flat\n",
    "\n",
    "def p_value_to_stars(p_value):\n",
    "    if p_value < 0.001:\n",
    "        return \"***\"\n",
    "    elif p_value < 0.01:\n",
    "        return \"**\"\n",
    "    elif p_value < 0.05:\n",
    "        return \"*\"\n",
    "    else:\n",
    "        return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "factual = [\"MMLU-Pro\", \"GPQA-Diamond\", \"ARC\"]\n",
    "opinion = [\"OpinionQA\", \"GlobalOpinionQA\", \"SIQA\"]\n",
    "method_list = ['grp_count', 'grp_ratio', 'grp_list', 'grp_disc', 'grp_reason']\n",
    "\n",
    "table2 = pd.DataFrame(columns = [\"Method\", \n",
    "                                 'Mean Flip Rate - Factual', 'Mean Flip Rate - Opinion',\n",
    "                                 '#A - Factual', '#A - Opinion',\n",
    "                                 '#D - Factual', '#D - Opinion',\n",
    "                                 '#A - D - Factual', '#A - D - Opinion',])\n",
    "\n",
    "for method in method_list:\n",
    "    target_row = [method]\n",
    "    factual_results = []\n",
    "    opinion_results = []\n",
    "    for benchmark in factual:\n",
    "        with open(f'../data/{benchmark}/sample_results/{method}_10.pkl', 'rb') as f:\n",
    "            factual_results.append(pickle.load(f)['data'])\n",
    "    for benchmark in opinion:\n",
    "        with open(f'../data/{benchmark}/sample_results/{method}_10.pkl', 'rb') as f:\n",
    "            opinion_results.append(pickle.load(f)['data'])\n",
    "\n",
    "    factual_results = flatten(factual_results)\n",
    "    opinion_results = flatten(opinion_results)\n",
    "\n",
    "    # Mean\n",
    "    for results in [factual_results, opinion_results]:\n",
    "        flip_list = [int(x['r'] != x['r^org']) for x in results]\n",
    "        mean_flip_rate = np.mean(flip_list)\n",
    "        target_row.append(round(mean_flip_rate, 4))\n",
    "\n",
    "    # number of agree\n",
    "    for results in [factual_results, opinion_results]:\n",
    "        flip_list = [int(x['r'] != x['r^org']) for x in results]\n",
    "        try:\n",
    "            agree_size_list = [x['agree_size'] for x in results]\n",
    "        except:\n",
    "            agree_size_list = [round(x['group_size']*(1-x['disagree_ratio'])) for x in results]\n",
    "        r_value, p_value = pearsonr(flip_list, agree_size_list)\n",
    "        target_row.append(f\"{r_value:.2f}{p_value_to_stars(p_value)}\")\n",
    "\n",
    "    # number of disagree\n",
    "    for results in [factual_results, opinion_results]:\n",
    "        flip_list = [int(x['r'] != x['r^org']) for x in results]\n",
    "        try:\n",
    "            disagree_size_list = [x['disagree_size'] for x in results]\n",
    "        except:\n",
    "            disagree_size_list = [round(x['group_size']*x['disagree_ratio']) for x in results]\n",
    "        r_value, p_value = pearsonr(flip_list, disagree_size_list)\n",
    "        target_row.append(f\"{r_value:.2f}{p_value_to_stars(p_value)}\")\n",
    "\n",
    "    # number of agree - disagree\n",
    "    for results in [factual_results, opinion_results]:\n",
    "        flip_list = [int(x['r'] != x['r^org']) for x in results]\n",
    "        try:\n",
    "            agree_size_list = [x['agree_size'] for x in results]\n",
    "        except:\n",
    "            agree_size_list = [round(x['group_size']*(1-x['disagree_ratio'])) for x in results]\n",
    "        try:\n",
    "            disagree_size_list = [x['disagree_size'] for x in results]\n",
    "        except:\n",
    "            disagree_size_list = [round(x['group_size']*x['disagree_ratio']) for x in results]\n",
    "        agree_diff_list = np.array(agree_size_list) - np.array(disagree_size_list)\n",
    "        r_value, p_value = pearsonr(flip_list, agree_diff_list)\n",
    "        target_row.append(f\"{r_value:.2f}{p_value_to_stars(p_value)}\")\n",
    "\n",
    "    table2.loc[len(table2)] = target_row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Method</th>\n",
       "      <th>Mean Flip Rate - Factual</th>\n",
       "      <th>Mean Flip Rate - Opinion</th>\n",
       "      <th>#A - Factual</th>\n",
       "      <th>#A - Opinion</th>\n",
       "      <th>#D - Factual</th>\n",
       "      <th>#D - Opinion</th>\n",
       "      <th>#A - D - Factual</th>\n",
       "      <th>#A - D - Opinion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>grp_count</td>\n",
       "      <td>0.2154</td>\n",
       "      <td>0.2694</td>\n",
       "      <td>-0.17***</td>\n",
       "      <td>-0.36***</td>\n",
       "      <td>0.16***</td>\n",
       "      <td>0.31***</td>\n",
       "      <td>-0.23***</td>\n",
       "      <td>-0.47***</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>grp_ratio</td>\n",
       "      <td>0.2226</td>\n",
       "      <td>0.3046</td>\n",
       "      <td>-0.28***</td>\n",
       "      <td>-0.41***</td>\n",
       "      <td>0.25***</td>\n",
       "      <td>0.38***</td>\n",
       "      <td>-0.37***</td>\n",
       "      <td>-0.56***</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>grp_list</td>\n",
       "      <td>0.2093</td>\n",
       "      <td>0.2954</td>\n",
       "      <td>-0.17***</td>\n",
       "      <td>-0.22***</td>\n",
       "      <td>0.15***</td>\n",
       "      <td>0.22***</td>\n",
       "      <td>-0.23***</td>\n",
       "      <td>-0.31***</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>grp_disc</td>\n",
       "      <td>0.2088</td>\n",
       "      <td>0.2824</td>\n",
       "      <td>-0.11***</td>\n",
       "      <td>-0.24***</td>\n",
       "      <td>0.09***</td>\n",
       "      <td>0.23***</td>\n",
       "      <td>-0.14***</td>\n",
       "      <td>-0.33***</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>grp_reason</td>\n",
       "      <td>0.2967</td>\n",
       "      <td>0.2957</td>\n",
       "      <td>-0.05***</td>\n",
       "      <td>-0.11***</td>\n",
       "      <td>0.05***</td>\n",
       "      <td>0.11***</td>\n",
       "      <td>-0.07***</td>\n",
       "      <td>-0.15***</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Method  Mean Flip Rate - Factual  Mean Flip Rate - Opinion  \\\n",
       "0   grp_count                    0.2154                    0.2694   \n",
       "1   grp_ratio                    0.2226                    0.3046   \n",
       "2    grp_list                    0.2093                    0.2954   \n",
       "3    grp_disc                    0.2088                    0.2824   \n",
       "4  grp_reason                    0.2967                    0.2957   \n",
       "\n",
       "  #A - Factual #A - Opinion #D - Factual #D - Opinion #A - D - Factual  \\\n",
       "0     -0.17***     -0.36***      0.16***      0.31***         -0.23***   \n",
       "1     -0.28***     -0.41***      0.25***      0.38***         -0.37***   \n",
       "2     -0.17***     -0.22***      0.15***      0.22***         -0.23***   \n",
       "3     -0.11***     -0.24***      0.09***      0.23***         -0.14***   \n",
       "4     -0.05***     -0.11***      0.05***      0.11***         -0.07***   \n",
       "\n",
       "  #A - D - Opinion  \n",
       "0         -0.47***  \n",
       "1         -0.56***  \n",
       "2         -0.31***  \n",
       "3         -0.33***  \n",
       "4         -0.15***  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Figure 1: Format of Peer Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plot\n",
    "importlib.reload(plot)\n",
    "import plot\n",
    "\n",
    "benchmarks = [\"MMLU-Pro\", \"GPQA-Diamond\", \"ARC\", \"OpinionQA\", \"GlobalOpinionQA\", \"SIQA\"]\n",
    "method_list = ['grp_count', 'grp_ratio', 'grp_list', 'grp_disc', 'grp_reason']\n",
    "title = ['Count', 'Ratio', 'List', 'Disc', 'Reason']\n",
    "list_PATH = [[f'../data/{benchmark}/sample_results/{method}_10.pkl' for benchmark in benchmarks] for method in method_list]\n",
    "plot.plot_from_list(list_PATH, metric = 'flip_rate', list_title = title, vlimit=(0,0.7))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Figure 2: Order of Peer Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plot\n",
    "importlib.reload(plot)\n",
    "import plot\n",
    "\n",
    "method_list = ['grp_count', 'grp_ratio', 'grp_list', 'grp_disc', 'grp_reason']\n",
    "list_data = []\n",
    "for method in method_list:\n",
    "    with open(f'../data/MMLU-Pro/sample_results/{method}_10.pkl', 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "        list_data.append(data['data'])\n",
    "\n",
    "\n",
    "list_ad_data = [[[] for _ in range(10)] for _ in range(10)]\n",
    "list_da_data = [[[] for _ in range(10)] for _ in range(10)]\n",
    "\n",
    "for target_data in list_data:\n",
    "    for r_idx, row in enumerate(target_data):\n",
    "        for c_idx, col in enumerate(row):\n",
    "            for ele in col:\n",
    "                if ele['order'] == 'ad':\n",
    "                    list_ad_data[r_idx][c_idx].append(ele)\n",
    "                elif ele['order'] == 'da':\n",
    "                    list_da_data[r_idx][c_idx].append(ele)\n",
    "                else:\n",
    "                    raise ValueError(\"Invalid order value. Expected 'ad' or 'da'.\")\n",
    "\n",
    "plot.plot_from_list_data([list_ad_data, list_da_data], metric = 'flip_rate', list_title = [\"Agree First\", \"Disagree First\"], vlimit=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "flatten_ad_data = flatten(list_ad_data)\n",
    "flatten_da_data = flatten(list_da_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Flip Rate - Agree First: 0.23\n",
      "Mean Flip Rate - Disagree First: 0.30\n"
     ]
    }
   ],
   "source": [
    "flip_rate_ad = np.array([int(x['r'] != x['r^org']) for x in flatten_ad_data])\n",
    "flip_rate_da = np.array([int(x['r'] != x['r^org']) for x in flatten_da_data])\n",
    "\n",
    "print(f\"Mean Flip Rate - Agree First: {np.mean(flip_rate_ad):.2f}\")\n",
    "print(f\"Mean Flip Rate - Disagree First: {np.mean(flip_rate_da):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C3: Modifying the Strength of Herd Behavior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MMLU-Pro 500 4746\n",
      "GPQA-Diamond 109 109\n",
      "ARC 30 30\n",
      "OpinionQA 216 216\n",
      "GlobalOpinionQA 500 552\n",
      "SIQA 142 142\n"
     ]
    }
   ],
   "source": [
    "benchmarks = [\"MMLU-Pro\", \"GPQA-Diamond\", \"ARC\", \"OpinionQA\", \"GlobalOpinionQA\", \"SIQA\"]\n",
    "for benchmark in benchmarks:\n",
    "    with open(f'../data/{benchmark}/results/org.pkl', 'rb') as f:\n",
    "        res_org = pickle.load(f)\n",
    "    res_org = [x for x in res_org if x['p_r^org'] < 0.8]\n",
    "    sample_size = min(500, len(res_org))\n",
    "    print(benchmark, sample_size, len(res_org))\n",
    "    # sample_res_org = random.Random(benchmark).sample(res_org, sample_size)\n",
    "    # mas_org = qa.qa_eval_mas_org(sample_res_org, agent_count = 5)\n",
    "    # with open(f'../data/{benchmark}/sample_results/mas_org_filtered.pkl', 'wb') as f:\n",
    "    #     pickle.dump(mas_org, f)\n",
    "    # list_data.append(mas_org)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MMLU-Pro 500 4746\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:   0%|                                          | 0/2500 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 100%|███████████████████████████████| 2500/2500 [08:10<00:00,  5.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GlobalOpinionQA 500 552\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 100%|███████████████████████████████| 2500/2500 [03:01<00:00, 13.76it/s]\n"
     ]
    }
   ],
   "source": [
    "# org baseline\n",
    "\n",
    "benchmarks = [\"MMLU-Pro\", \"GlobalOpinionQA\"]\n",
    "for benchmark in benchmarks:\n",
    "    with open(f'../data/{benchmark}/results/org.pkl', 'rb') as f:\n",
    "        res_org = pickle.load(f)\n",
    "    res_org = [x for x in res_org if x['p_r^org'] < 0.8]\n",
    "    sample_size = min(500, len(res_org))\n",
    "    print(benchmark, sample_size, len(res_org))\n",
    "    sample_res_org = random.Random(benchmark).sample(res_org, sample_size)\n",
    "    mas_org = qa.qa_eval_mas_org(sample_res_org, agent_count = 5)\n",
    "    with open(f'../data/{benchmark}/sample_results/mas_org_filtered.pkl', 'wb') as f:\n",
    "        pickle.dump(mas_org, f)\n",
    "    list_data.append(mas_org)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 100%|███████████████████████████████| 2500/2500 [03:44<00:00, 11.14it/s]\n"
     ]
    }
   ],
   "source": [
    "# cot baseline\n",
    "\n",
    "benchmarks = [\"MMLU-Pro\", \"GlobalOpinionQA\"]\n",
    "for benchmark in benchmarks:\n",
    "    with open(f'../data/{benchmark}/results/org.pkl', 'rb') as f:\n",
    "        res_org = pickle.load(f)\n",
    "    res_org = [x for x in res_org if x['p_r^org'] < 0.8]\n",
    "    sample_size = min(500, len(res_org))\n",
    "    sample_res_org = random.Random(benchmark).sample(res_org, sample_size)\n",
    "    mas_org = qa.qa_eval_mas_cot(sample_res_org, agent_count = 5)\n",
    "    with open(f'../data/{benchmark}/sample_results/mas_org_cot.pkl', 'wb') as f:\n",
    "        pickle.dump(mas_org, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from collections import Counter\n",
    "\n",
    "def calculate_entropy(answers):\n",
    "    n = len(answers)\n",
    "    if n == 0:\n",
    "        return 0.0\n",
    "\n",
    "    counts = Counter(answers)\n",
    "    entropy = 0.0\n",
    "    for count in counts.values():\n",
    "        p = count / n\n",
    "        entropy -= p * math.log2(p)\n",
    "\n",
    "    return entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entropy for MMLU-Pro: 1.10 ± 0.60\n",
      "Entropy for GlobalOpinionQA: 0.82 ± 0.41\n"
     ]
    }
   ],
   "source": [
    "# Filtered with p_r^org < 0.8, capped with 500 examples per dataset\n",
    "\n",
    "benchmarks = [\"MMLU-Pro\", \"GlobalOpinionQA\"]\n",
    "for benchmark in benchmarks:\n",
    "    entropy = []\n",
    "    with open(f'../data/{benchmark}/sample_results/mas_org_filtered.pkl', 'rb') as f:\n",
    "        mas_org = pickle.load(f)\n",
    "    for peer_info in mas_org:\n",
    "        peer_opinions = []\n",
    "        for peer in peer_info:\n",
    "            peer_opinions.append(peer_info[peer]['r^org'])\n",
    "        entropy.append(calculate_entropy(peer_opinions))\n",
    "\n",
    "    print(f\"Entropy for {benchmark}: {np.mean(entropy):.2f} ± {np.std(entropy):.2f}\")\n",
    "        \n",
    "# result of full dataset, capped with 200 example:\n",
    "#\n",
    "# Entropy for MMLU-Pro: 0.54 ± 0.64\n",
    "# Entropy for GPQA-Diamond: 0.60 ± 0.57\n",
    "# Entropy for ARC: 0.06 ± 0.23\n",
    "# Entropy for OpinionQA: 0.19 ± 0.39\n",
    "# Entropy for GlobalOpinionQA: 0.27 ± 0.42\n",
    "# Entropy for SIQA: 0.07 ± 0.22"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# benchmarks = [\"MMLU-Pro\", \"GlobalOpinionQA\"]\n",
    "# list_data = []\n",
    "# for benchmark in benchmarks:\n",
    "#     with open(f'../data/{benchmark}/sample_results/mas_org_filtered.pkl', 'rb') as f:\n",
    "#         mas_org = pickle.load(f)\n",
    "#     mas_res = qa.qa_eval_mas_res(mas_org)\n",
    "#     with open(f'../data/{benchmark}/sample_results/mas_res_filtered.pkl', 'wb') as f:\n",
    "#         pickle.dump(mas_res, f)\n",
    "#     list_data.append(mas_res)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmarks = [\"MMLU-Pro\", \"GlobalOpinionQA\"]\n",
    "list_data = []\n",
    "for benchmark in benchmarks:\n",
    "    with open(f'../data/{benchmark}/sample_results/mas_res_filtered.pkl', 'rb') as f:\n",
    "        mas_res = pickle.load(f)\n",
    "    with open(f'../data/{benchmark}/sample_results/mas_org_filtered.pkl', 'rb') as f:\n",
    "        mas_org = pickle.load(f)\n",
    "    with open(f'../data/{benchmark}/sample_results/mas_org_cot.pkl', 'rb') as f:\n",
    "        mas_org_cot = pickle.load(f)\n",
    "    mas_res[\"original\"] = mas_org\n",
    "    mas_res[\"cot\"] = mas_org_cot\n",
    "\n",
    "    list_data.append(mas_res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table 3: Controllable herd behavior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def has_majority(answers):\n",
    "    if not answers:\n",
    "        return False\n",
    "    count = Counter(answers)\n",
    "    majority_threshold = len(answers) // 2 + 1  # more than half\n",
    "    return any(v >= majority_threshold for v in count.values())\n",
    "\n",
    "# Example usage:\n",
    "print(has_majority(['A', 'A', 'B', 'C']))  # False\n",
    "print(has_majority(['A', 'A', 'A', 'B']))  # True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "def has_unanimous(answers):\n",
    "    if not answers:\n",
    "        return False\n",
    "    return all(ans == answers[0] for ans in answers)\n",
    "\n",
    "# Example usage:\n",
    "print(has_unanimous(['A', 'A', 'A']))     # True\n",
    "print(has_unanimous(['A', 'B', 'A']))     # False\n",
    "print(has_unanimous([]))                 # False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A\n",
      "None\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def get_unique_plurality(answers):\n",
    "    if not answers:\n",
    "        return None\n",
    "    count = Counter(answers)\n",
    "    most_common = count.most_common()\n",
    "    \n",
    "    if len(most_common) == 1:\n",
    "        return most_common[0][0]  # Only one answer exists\n",
    "    \n",
    "    if most_common[0][1] > most_common[1][1]:\n",
    "        return most_common[0][0]  # Unique highest count\n",
    "    else:\n",
    "        return None  # Tie for highest count\n",
    "\n",
    "# Example usage:\n",
    "print(get_unique_plurality(['A', 'A', 'B', 'C']))   # A\n",
    "print(get_unique_plurality(['A', 'B', 'C']))        # None (all equal)\n",
    "print(get_unique_plurality(['A', 'B', 'B', 'A']))    # None (tie between A and B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A\n",
      "None\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def get_majority(answers):\n",
    "    if not answers:\n",
    "        return None\n",
    "    count = Counter(answers)\n",
    "    n = len(answers)\n",
    "    for ans, freq in count.items():\n",
    "        if freq > n // 2:\n",
    "            return ans\n",
    "    return None  # No majority\n",
    "\n",
    "# Example usage:\n",
    "print(get_majority(['A', 'A', 'B', 'A']))   # A (3 out of 4)\n",
    "print(get_majority(['A', 'B', 'C']))        # None (no majority)\n",
    "print(get_majority(['B', 'B', 'A', 'A']))   # None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A\n",
      "None\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "def get_unanimous(answers):\n",
    "    if not answers:\n",
    "        return None\n",
    "    first = answers[0]\n",
    "    if all(ans == first for ans in answers):\n",
    "        return first\n",
    "    return None  # Not unanimous\n",
    "\n",
    "# Example usage:\n",
    "print(get_unanimous(['A', 'A', 'A']))       # A\n",
    "print(get_unanimous(['A', 'A', 'B']))       # None\n",
    "print(get_unanimous([]))                   # None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Method</th>\n",
       "      <th>MMLU-Pro Flip Rate</th>\n",
       "      <th>MMLU-Pro Entropy</th>\n",
       "      <th>MMLU-Pro Consensus Rate</th>\n",
       "      <th>MMLU-Pro Accuracy</th>\n",
       "      <th>GlobalOpinionQA Flip Rate</th>\n",
       "      <th>GlobalOpinionQA Entropy</th>\n",
       "      <th>GlobalOpinionQA Consensus Rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>original</td>\n",
       "      <td>-</td>\n",
       "      <td>1.10</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.04</td>\n",
       "      <td>-</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>cot</td>\n",
       "      <td>-</td>\n",
       "      <td>0.91</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.23</td>\n",
       "      <td>-</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>control</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>strong_factors</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.54</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.59</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>weak_factors</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>strong_prompt</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>weak_prompt</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.61</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Method MMLU-Pro Flip Rate MMLU-Pro Entropy MMLU-Pro Consensus Rate  \\\n",
       "0        original                  -             1.10                    0.13   \n",
       "1             cot                  -             0.91                    0.28   \n",
       "2         control               0.55             0.43                    0.56   \n",
       "3  strong_factors               0.63             0.43                    0.54   \n",
       "4    weak_factors               0.36             0.28                    0.69   \n",
       "5   strong_prompt               0.55             0.43                    0.57   \n",
       "6     weak_prompt               0.55             0.43                    0.56   \n",
       "\n",
       "  MMLU-Pro Accuracy GlobalOpinionQA Flip Rate GlobalOpinionQA Entropy  \\\n",
       "0              0.04                         -                    0.82   \n",
       "1              0.23                         -                    0.63   \n",
       "2              0.18                      0.44                    0.29   \n",
       "3              0.29                      0.59                    0.49   \n",
       "4              0.16                      0.23                    0.22   \n",
       "5              0.17                      0.44                    0.29   \n",
       "6              0.18                      0.46                    0.36   \n",
       "\n",
       "  GlobalOpinionQA Consensus Rate  \n",
       "0                           0.14  \n",
       "1                           0.34  \n",
       "2                           0.69  \n",
       "3                           0.46  \n",
       "4                           0.76  \n",
       "5                           0.69  \n",
       "6                           0.61  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table3_mmlu = pd.DataFrame(columns = [\"Method\",\n",
    "                                    \"MMLU-Pro Flip Rate\",\n",
    "                                    \"MMLU-Pro Entropy\", \n",
    "                                    \"MMLU-Pro Consensus Rate\",\n",
    "                                    \"MMLU-Pro Accuracy\"])\n",
    "\n",
    "table3_glo = pd.DataFrame(columns = [\"Method\",\n",
    "                                    \"GlobalOpinionQA Flip Rate\",\n",
    "                                    \"GlobalOpinionQA Entropy\",\n",
    "                                    \"GlobalOpinionQA Consensus Rate\"])\n",
    "\n",
    "for dataset, benchmark in zip(list_data, [\"MMLU-Pro\", \"GlobalOpinionQA\"]):\n",
    "    methods = list(dataset.keys())\n",
    "    methods.remove(\"cot\")\n",
    "    methods = [\"cot\"] + methods\n",
    "    methods.remove(\"original\")\n",
    "    methods = [\"original\"] + methods\n",
    "    for method in methods:\n",
    "        target_row = [method]\n",
    "        list_flip_rate = []\n",
    "        list_entropy = []\n",
    "        list_consensus_rate = []\n",
    "        list_accuracy = []\n",
    "        for ele in dataset[method]:\n",
    "            if method not in [\"original\", \"cot\"]:\n",
    "                flip_rate = np.mean([x['r'] != x['r^org'] for x in ele.values()])\n",
    "                list_flip_rate.append(flip_rate)\n",
    "\n",
    "            if method in [\"original\", \"cot\"]:\n",
    "                peer_opinions = [x['r^org'] for x in ele.values()]\n",
    "            else:\n",
    "                peer_opinions = [x['r'] for x in ele.values()]\n",
    "\n",
    "            entropy = calculate_entropy(peer_opinions)\n",
    "            list_entropy.append(entropy)\n",
    "\n",
    "            bool_consensus = int(has_unanimous(peer_opinions))\n",
    "            list_consensus_rate.append(bool_consensus)\n",
    "\n",
    "            if benchmark == \"MMLU-Pro\":\n",
    "                plularity_answer = get_unanimous(peer_opinions)\n",
    "                list_accuracy.append(int(plularity_answer == ele[\"A\"]['answer']))\n",
    "\n",
    "\n",
    "        if method in [\"original\", \"cot\"]:\n",
    "            target_row.append(\"-\")\n",
    "        else:\n",
    "            target_row.append(f\"{np.mean(list_flip_rate):.2f}\") #  ± {np.std(list_flip_rate):.2f}\n",
    "        target_row.append(f\"{np.mean(list_entropy):.2f}\") #  ± {np.std(list_entropy):.2f}\n",
    "        target_row.append(f\"{np.mean(list_consensus_rate):.2f}\") #  ± {np.std(list_consensus_rate):.2f}\n",
    "        if benchmark == \"MMLU-Pro\":\n",
    "            target_row.append(f\"{np.mean(list_accuracy):.2f}\") #  ± {np.std(list_accuracy):.2f}\n",
    "            table3_mmlu.loc[len(table3_mmlu)] = target_row\n",
    "        else:\n",
    "            table3_glo.loc[len(table3_glo)] = target_row\n",
    "        \n",
    "table3 = table3_mmlu.merge(table3_glo, how='inner', on='Method')\n",
    "table3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import ttest_rel\n",
    "import numpy as np\n",
    "\n",
    "def get_metric_lists(dataset, benchmark, method):\n",
    "    flip_rate, entropy, majority, acc = [], [], [], []\n",
    "    for ele in dataset[method]:\n",
    "        opinions = [x['r'] if method != \"original\" else x['r^org'] for x in ele.values()]\n",
    "\n",
    "        if method != \"original\":\n",
    "            flip_rate.append(np.mean([x['r'] != x['r^org'] for x in ele.values()]))\n",
    "\n",
    "        entropy.append(calculate_entropy(opinions))\n",
    "        majority.append(int(has_majority(opinions)))\n",
    "\n",
    "        if benchmark == \"MMLU-Pro\":\n",
    "            acc.append(int(get_unique_plurality(opinions) == ele[\"A\"]['answer']))\n",
    "\n",
    "    return flip_rate, entropy, majority, acc\n",
    "\n",
    "def significance_vs_second_best(dataset, benchmark, alpha=0.05):\n",
    "    \"\"\"\n",
    "    Compare each method against the second-best performer (per metric).\n",
    "    \"\"\"\n",
    "    all_stats = {}\n",
    "    flip_dict, entropy_dict, majority_dict, acc_dict = {}, {}, {}, {}\n",
    "\n",
    "    methods = list(dataset.keys())\n",
    "\n",
    "    # Collect all metric values\n",
    "    for method in methods:\n",
    "        flip, entropy, majority, acc = get_metric_lists(dataset, benchmark, method)\n",
    "        if method != \"original\":\n",
    "            flip_dict[method] = flip\n",
    "        entropy_dict[method] = entropy\n",
    "        majority_dict[method] = majority\n",
    "        if benchmark == \"MMLU-Pro\":\n",
    "            acc_dict[method] = acc\n",
    "\n",
    "    def evaluate(metric_dict, goal=\"max\"):\n",
    "        # Compute mean for all methods\n",
    "        metric_mean = {k: np.mean(v) for k, v in metric_dict.items()}\n",
    "        # Sort based on goal\n",
    "        reverse = (goal == \"max\")\n",
    "        sorted_methods = sorted(metric_mean.items(), key=lambda x: x[1], reverse=reverse)\n",
    "        best, second_best = sorted_methods[0][0], sorted_methods[1][0]\n",
    "        sig_results = {}\n",
    "\n",
    "        for method in metric_dict:\n",
    "            if method == second_best:\n",
    "                sig_results[method] = (metric_mean[method], \"Second Best\")\n",
    "                continue\n",
    "\n",
    "            t_stat, p_val = ttest_rel(metric_dict[method], metric_dict[second_best])\n",
    "            sig = (p_val < alpha and\n",
    "                   ((goal == \"max\" and np.mean(metric_dict[method]) > np.mean(metric_dict[second_best])) or\n",
    "                    (goal == \"min\" and np.mean(metric_dict[method]) < np.mean(metric_dict[second_best]))))\n",
    "            sig_results[method] = (np.mean(metric_dict[method]), sig)\n",
    "\n",
    "        return sig_results\n",
    "\n",
    "    all_stats['flip_rate'] = evaluate(flip_dict, goal=\"max\")\n",
    "    all_stats['entropy'] = evaluate(entropy_dict, goal=\"min\")\n",
    "    all_stats['majority_rate'] = evaluate(majority_dict, goal=\"max\")\n",
    "    if benchmark == \"MMLU-Pro\":\n",
    "        all_stats['accuracy'] = evaluate(acc_dict, goal=\"max\")\n",
    "\n",
    "    return all_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "mmlu_stats = significance_vs_second_best(list_data[0], benchmark=\"MMLU-Pro\")\n",
    "glo_stats = significance_vs_second_best(list_data[1], benchmark=\"GlobalOpinionQA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'flip_rate': {'control': (np.float64(0.44160000000000005), np.False_),\n",
       "  'strong_factors': (np.float64(0.5872), np.True_),\n",
       "  'weak_factors': (np.float64(0.2332), False),\n",
       "  'strong_prompt': (np.float64(0.444), np.False_),\n",
       "  'weak_prompt': (np.float64(0.458), 'Second Best')},\n",
       " 'entropy': {'control': (np.float64(0.29338014670376605), 'Second Best'),\n",
       "  'strong_factors': (np.float64(0.4934225060981175), np.False_),\n",
       "  'weak_factors': (np.float64(0.2164225403259173), np.True_),\n",
       "  'strong_prompt': (np.float64(0.29448210170463146), np.False_),\n",
       "  'weak_prompt': (np.float64(0.35670311428090884), np.False_),\n",
       "  'original': (np.float64(0.8247411952039331), np.False_)},\n",
       " 'majority_rate': {'control': (np.float64(0.99), np.False_),\n",
       "  'strong_factors': (np.float64(0.98), np.False_),\n",
       "  'weak_factors': (np.float64(0.994), np.False_),\n",
       "  'strong_prompt': (np.float64(0.986), np.False_),\n",
       "  'weak_prompt': (np.float64(0.992), 'Second Best'),\n",
       "  'original': (np.float64(0.934), False)}}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glo_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix: Different LLMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# benchmarks = [\"MMLU-Pro\", \"GPQA-Diamond\", \"ARC\", \"OpinionQA\", \"GlobalOpinionQA\", \"SIQA\"]\n",
    "# llms = ['gpt-4o-mini', 'gpt-4o', 'gpt-4.1', 'gpt-4.1-mini', 'gpt-4.1-nano']\n",
    "# type_names = ['2nd', 'lst']\n",
    "\n",
    "# for benchmark in benchmarks:\n",
    "#     with open(f'../data/{benchmark}/sample_results/org.pkl', 'rb') as f:\n",
    "#         res_org = pickle.load(f)\n",
    "\n",
    "#     for llm in llms:\n",
    "#         res_org = qa.qa_eval_org(res_org, llm = llm)\n",
    "#         with open(f'../data/{benchmark}/sample_results/org_{llm}.pkl', 'wb') as f:\n",
    "#             pickle.dump(res_org, f)\n",
    "#         for disagree_type in type_names:\n",
    "#             results = qa.qa_eval_one(res_org, disagree_type, llm = llm)\n",
    "\n",
    "#             with open(f'../data/{benchmark}/sample_results/one_{disagree_type}_{llm}.pkl', 'wb') as f:\n",
    "#                 pickle.dump(results, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 100%|██████████| 200/200 [00:01<00:00, 190.00it/s]\n",
      "Processing: 100%|██████████| 200/200 [00:01<00:00, 130.90it/s]\n",
      "Processing: 100%|██████████| 200/200 [00:01<00:00, 126.33it/s]\n",
      "Processing: 100%|██████████| 198/198 [00:01<00:00, 115.92it/s]\n",
      "Processing: 100%|██████████| 198/198 [00:01<00:00, 139.61it/s]\n",
      "Processing: 100%|██████████| 198/198 [00:01<00:00, 133.83it/s]\n",
      "Processing: 100%|██████████| 200/200 [00:01<00:00, 168.57it/s]\n",
      "Processing: 100%|██████████| 200/200 [00:01<00:00, 194.84it/s]\n",
      "Processing: 100%|██████████| 200/200 [00:01<00:00, 166.12it/s]\n",
      "Processing: 100%|██████████| 200/200 [00:01<00:00, 170.17it/s]\n",
      "Processing: 100%|██████████| 200/200 [00:01<00:00, 196.29it/s]\n",
      "Processing: 100%|██████████| 200/200 [00:01<00:00, 187.47it/s]\n",
      "Processing: 100%|██████████| 200/200 [00:01<00:00, 151.10it/s]\n",
      "Processing: 100%|██████████| 200/200 [00:01<00:00, 189.12it/s]\n",
      "Processing: 100%|██████████| 200/200 [00:01<00:00, 164.27it/s]\n",
      "Processing: 100%|██████████| 200/200 [00:01<00:00, 158.46it/s]\n",
      "Processing: 100%|██████████| 200/200 [00:01<00:00, 167.70it/s]\n",
      "Processing: 100%|██████████| 200/200 [00:01<00:00, 166.11it/s]\n"
     ]
    }
   ],
   "source": [
    "benchmarks = [\"MMLU-Pro\", \"GPQA-Diamond\", \"ARC\", \"OpinionQA\", \"GlobalOpinionQA\", \"SIQA\"]\n",
    "llms = ['llama-3.3-70B']\n",
    "type_names = ['2nd', 'lst']\n",
    "\n",
    "for benchmark in benchmarks:\n",
    "    with open(f'../data/{benchmark}/sample_results/org.pkl', 'rb') as f:\n",
    "        res_org = pickle.load(f)\n",
    "\n",
    "    for llm in llms:\n",
    "        res_org = qa.qa_eval_org(res_org, llm = llm)\n",
    "        with open(f'../data/{benchmark}/sample_results/org_{llm}.pkl', 'wb') as f:\n",
    "            pickle.dump(res_org, f)\n",
    "        for disagree_type in type_names:\n",
    "            results = qa.qa_eval_one(res_org, disagree_type, llm = llm)\n",
    "\n",
    "            with open(f'../data/{benchmark}/sample_results/one_{disagree_type}_{llm}.pkl', 'wb') as f:\n",
    "                pickle.dump(results, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmarks = [\"MMLU-Pro\", \"GPQA-Diamond\", \"ARC\", \"OpinionQA\", \"GlobalOpinionQA\", \"SIQA\"]\n",
    "llms = ['gpt-4o-mini', 'gpt-4o', 'gpt-4.1', 'gpt-4.1-mini', 'gpt-4.1-nano', 'llama-3.3-70B', 'llama-3.2-3B', 'qwen-2.5-72B']\n",
    "disagree_types = ['2nd', 'lst']\n",
    "\n",
    "dict_results = {}\n",
    "dict_results['Average'] = {f\"{llm}_{disagree_type}\":[] for llm in llms for disagree_type in disagree_types}\n",
    "for benchmark in benchmarks:\n",
    "    dict_results[benchmark] = {}\n",
    "    for llm in llms:\n",
    "        temp_inputs = {}\n",
    "        for disagree_type in disagree_types:\n",
    "            PATH = f'../data/{benchmark}/sample_results/one_{disagree_type}_{llm}.pkl'\n",
    "            with open(PATH, 'rb') as f:\n",
    "                retrieved_results = pickle.load(f)\n",
    "                retrieved_results = [x for x in retrieved_results if x['r'] != None]\n",
    "                temp_inputs[disagree_type] = retrieved_results\n",
    "\n",
    "        unique_ids = set([x['question_id'] for x in temp_inputs[disagree_types[0]]])\n",
    "        for disagree_type in disagree_types:\n",
    "            unique_ids = unique_ids & set([x['question_id'] for x in temp_inputs[disagree_type]])\n",
    "\n",
    "        for disagree_type in disagree_types:\n",
    "            valid_results = [x for x in temp_inputs[disagree_type] if x['question_id'] in unique_ids]\n",
    "            dict_results[benchmark][f\"{llm}_{disagree_type}\"] = valid_results\n",
    "            dict_results['Average'][f\"{llm}_{disagree_type}\"].extend(valid_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Method</th>\n",
       "      <th>MMLU-Pro</th>\n",
       "      <th>GPQA-Diamond</th>\n",
       "      <th>ARC</th>\n",
       "      <th>OpinionQA</th>\n",
       "      <th>GlobalOpinionQA</th>\n",
       "      <th>SIQA</th>\n",
       "      <th>Average</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>gpt-4o-mini_2nd</td>\n",
       "      <td>0.5*</td>\n",
       "      <td>0.59*</td>\n",
       "      <td>0.11*</td>\n",
       "      <td>0.64*</td>\n",
       "      <td>0.71*</td>\n",
       "      <td>0.16*</td>\n",
       "      <td>0.45*</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gpt-4o-mini_lst</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.54</td>\n",
       "      <td>0.59</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>gpt-4o_2nd</td>\n",
       "      <td>0.55*</td>\n",
       "      <td>0.7*</td>\n",
       "      <td>0.06*</td>\n",
       "      <td>0.52*</td>\n",
       "      <td>0.57*</td>\n",
       "      <td>0.26*</td>\n",
       "      <td>0.44*</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>gpt-4o_lst</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>gpt-4.1_2nd</td>\n",
       "      <td>0.51*</td>\n",
       "      <td>0.63*</td>\n",
       "      <td>0.05*</td>\n",
       "      <td>0.66*</td>\n",
       "      <td>0.66*</td>\n",
       "      <td>0.22*</td>\n",
       "      <td>0.45*</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>gpt-4.1_lst</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.54</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>gpt-4.1-mini_2nd</td>\n",
       "      <td>0.46*</td>\n",
       "      <td>0.49*</td>\n",
       "      <td>0.04*</td>\n",
       "      <td>0.4*</td>\n",
       "      <td>0.42*</td>\n",
       "      <td>0.2*</td>\n",
       "      <td>0.33*</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>gpt-4.1-mini_lst</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>gpt-4.1-nano_2nd</td>\n",
       "      <td>0.62*</td>\n",
       "      <td>0.57*</td>\n",
       "      <td>0.18*</td>\n",
       "      <td>0.51*</td>\n",
       "      <td>0.62*</td>\n",
       "      <td>0.33*</td>\n",
       "      <td>0.47*</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>gpt-4.1-nano_lst</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>llama-3.3-70B_2nd</td>\n",
       "      <td>0.43*</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.32*</td>\n",
       "      <td>0.46*</td>\n",
       "      <td>0.2*</td>\n",
       "      <td>0.32*</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>llama-3.3-70B_lst</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>llama-3.2-3B_2nd</td>\n",
       "      <td>0.38*</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.18*</td>\n",
       "      <td>0.28*</td>\n",
       "      <td>0.48*</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.28*</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>llama-3.2-3B_lst</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>qwen-2.5-72B_2nd</td>\n",
       "      <td>0.46*</td>\n",
       "      <td>0.53*</td>\n",
       "      <td>0.05*</td>\n",
       "      <td>0.24*</td>\n",
       "      <td>0.32*</td>\n",
       "      <td>0.18*</td>\n",
       "      <td>0.29*</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>qwen-2.5-72B_lst</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Method MMLU-Pro GPQA-Diamond    ARC OpinionQA GlobalOpinionQA  \\\n",
       "0     gpt-4o-mini_2nd     0.5*        0.59*  0.11*     0.64*           0.71*   \n",
       "1     gpt-4o-mini_lst     0.25         0.42   0.06      0.54            0.59   \n",
       "2          gpt-4o_2nd    0.55*         0.7*  0.06*     0.52*           0.57*   \n",
       "3          gpt-4o_lst     0.34         0.45   0.02      0.34            0.43   \n",
       "4         gpt-4.1_2nd    0.51*        0.63*  0.05*     0.66*           0.66*   \n",
       "5         gpt-4.1_lst     0.23         0.35   0.02      0.49            0.54   \n",
       "6    gpt-4.1-mini_2nd    0.46*        0.49*  0.04*      0.4*           0.42*   \n",
       "7    gpt-4.1-mini_lst      0.3         0.32    0.0      0.25            0.33   \n",
       "8    gpt-4.1-nano_2nd    0.62*        0.57*  0.18*     0.51*           0.62*   \n",
       "9    gpt-4.1-nano_lst     0.46         0.39   0.12      0.42            0.52   \n",
       "10  llama-3.3-70B_2nd    0.43*          0.4   0.13     0.32*           0.46*   \n",
       "11  llama-3.3-70B_lst     0.27         0.42    0.1      0.15            0.29   \n",
       "12   llama-3.2-3B_2nd    0.38*         0.22  0.18*     0.28*           0.48*   \n",
       "13   llama-3.2-3B_lst     0.29         0.19   0.12       0.2            0.34   \n",
       "14   qwen-2.5-72B_2nd    0.46*        0.53*  0.05*     0.24*           0.32*   \n",
       "15   qwen-2.5-72B_lst     0.16         0.24   0.02      0.09            0.18   \n",
       "\n",
       "     SIQA Average  \n",
       "0   0.16*   0.45*  \n",
       "1    0.06    0.32  \n",
       "2   0.26*   0.44*  \n",
       "3    0.14    0.29  \n",
       "4   0.22*   0.45*  \n",
       "5     0.1    0.29  \n",
       "6    0.2*   0.33*  \n",
       "7    0.11    0.22  \n",
       "8   0.33*   0.47*  \n",
       "9    0.16    0.34  \n",
       "10   0.2*   0.32*  \n",
       "11    0.1    0.22  \n",
       "12   0.12   0.28*  \n",
       "13    0.1    0.21  \n",
       "14  0.18*   0.29*  \n",
       "15   0.03    0.12  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "benchmarks = [\"MMLU-Pro\", \"GPQA-Diamond\", \"ARC\", \"OpinionQA\", \"GlobalOpinionQA\", \"SIQA\", \"Average\"]\n",
    "\n",
    "table1 = pd.DataFrame(columns=[\"Method\"] + benchmarks)\n",
    "\n",
    "for llm in llms:\n",
    "    for disagree_type in disagree_types:\n",
    "        row = [f\"{llm}_{disagree_type}\"]\n",
    "        for benchmark in benchmarks:\n",
    "            flip_rate = np.mean([int(r['r'] != r['r^org']) for r in dict_results[benchmark][f\"{llm}_{disagree_type}\"]])\n",
    "            row.append(round(flip_rate, 2))\n",
    "        table1.loc[len(table1)] = row\n",
    "\n",
    "# Run paired t-test and annotate _2nd values if significantly smaller\n",
    "for benchmark in benchmarks:\n",
    "    for llm in llms:\n",
    "        values_2nd = np.array([int(r['r'] != r['r^org']) for r in dict_results[benchmark][f\"{llm}_2nd\"]])\n",
    "        values_lst = np.array([int(r['r'] != r['r^org']) for r in dict_results[benchmark][f\"{llm}_lst\"]])\n",
    "    \n",
    "        t_stat, p_val = ttest_rel(values_2nd, values_lst)\n",
    "    \n",
    "        if p_val < 0.05:\n",
    "            table1[benchmark] = table1[benchmark].astype(object)\n",
    "            if np.mean(values_2nd) > np.mean(values_lst):\n",
    "                # If 2nd is significantly smaller, annotate with '*'\n",
    "                original_value = table1.loc[table1[\"Method\"] == f\"{llm}_2nd\", benchmark].tolist()[0]\n",
    "                table1.loc[table1[\"Method\"] == f\"{llm}_2nd\", benchmark] = f\"{original_value}*\"\n",
    "            else:\n",
    "                # If 2nd is significantly larger, annotate with '*'\n",
    "                original_value = table1.loc[table1[\"Method\"] == f\"{llm}_lst\", benchmark].tolist()[0]\n",
    "                table1.loc[table1[\"Method\"] == f\"{llm}_lst\", benchmark] = f\"{original_value}*\"\n",
    "\n",
    "table1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/GPQA-Diamond/sample_results/one_2nd_llama-3.2-3B.pkl', 'rb') as f:\n",
    "    temp_2nd = pickle.load(f)\n",
    "\n",
    "with open('../data/GPQA-Diamond/sample_results/one_lst_llama-3.2-3B.pkl', 'rb') as f:\n",
    "    temp_lst = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question_id': 19,\n",
       " 'category': 'Physics',\n",
       " 'question': 'A common approximation made in many-body nuclear calculations of some interaction (for example an electroweak interaction) is to suppose that nucleons in a nucleus behave in the same way as free nucleons. Which of the following four assumptions jointly imply this \"\"impulse\"\" approximation?\\n\\n1. The interaction current only interacts with individual nucleons.\\n2. The nucleus is transparent apart from the selected nucleon.\\n3. The quarks internal to the selected nucleon are non-relativistic.\\n4. The interaction proceeds as if the selected nucleon experiences no binding forces.',\n",
       " 'options': ['1,2,3', '2,3,4', '1,3,4', '1,2,4'],\n",
       " 'answer': 'D',\n",
       " 'r^org': 'B',\n",
       " 'p_r^org': np.float64(0.5482601189759819),\n",
       " 'topk^org': [('B', np.float64(0.5483)),\n",
       "  ('A', np.float64(0.2935)),\n",
       "  ('D', np.float64(0.0841)),\n",
       "  ('C', np.float64(0.0742))],\n",
       " 'r_j': 'A',\n",
       " 'p_r_j': np.float64(0.2935),\n",
       " 'r': 'B',\n",
       " 'p_r': np.float64(0.4577062127145336),\n",
       " 'topk': [('B', np.float64(0.4577)),\n",
       "  ('A', np.float64(0.4039)),\n",
       "  ('C', np.float64(0.0901)),\n",
       "  ('D', np.float64(0.0482))]}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_2nd[6]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DUMP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ETHICS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw = pd.read_csv('../data/ETHICS/raw/cm_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ethics_full = []\n",
    "for i, row in df_raw.iterrows():\n",
    "    input_ele = {}\n",
    "    input_ele['question_id'] = i\n",
    "    input_ele['question'] = \"Question: Would most people believe this reasonable or unreasonable to say?\\n\" + row['input'] \n",
    "    choices = ['reasonable', 'unreasonable']\n",
    "    random.Random(i).shuffle(choices)\n",
    "    input_ele['options'] = choices\n",
    "    label = row['label']\n",
    "    label_text = ['reasonable', 'unreasonable'][label]\n",
    "    true_label = choices.index(label_text)\n",
    "    answer = 'AB'[true_label]\n",
    "    input_ele['answer'] = answer\n",
    "\n",
    "    ethics_full.append(input_ele)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total number of questions in ETHICS: 3885\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing QA: 100%|██████████| 3885/3885 [01:20<00:00, 48.15it/s]\n"
     ]
    }
   ],
   "source": [
    "with open('../data/ETHICS/results/full.pkl', 'wb') as f:\n",
    "    pickle.dump(ethics_full, f)\n",
    "\n",
    "print(\"total number of questions in ETHICS:\", len(ethics_full))\n",
    "\n",
    "res_org = qa.qa_eval_org(ethics_full)\n",
    "\n",
    "with open('../data/ETHICS/results/org.pkl', 'wb') as f:\n",
    "    pickle.dump(res_org, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing QA: 100%|██████████| 3885/3885 [01:01<00:00, 62.89it/s] \n",
      "Processing QA: 100%|██████████| 3885/3885 [00:55<00:00, 70.63it/s] \n",
      "Processing QA: 100%|██████████| 3885/3885 [00:58<00:00, 66.36it/s] \n",
      "Processing QA: 100%|██████████| 3885/3885 [01:08<00:00, 56.37it/s]\n"
     ]
    }
   ],
   "source": [
    "with open('../data/ETHICS/results/org.pkl', 'rb') as f:\n",
    "    res_org = pickle.load(f)\n",
    "\n",
    "type_names = ['1st', '2nd', 'rnd', 'lst']\n",
    "\n",
    "for disagree_type in type_names:\n",
    "    results = qa.qa_eval_one(res_org, disagree_type)\n",
    "    with open(f'../data/ETHICS/results/one_{disagree_type}.pkl', 'wb') as f:\n",
    "        pickle.dump(results, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "herd_behavior",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
