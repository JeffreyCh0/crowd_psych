import json

import sys
sys.path.append('../src')
from agent import Agent, top_norm_prob
import numpy as np
import multiprocess as mp
from tqdm import tqdm
import random
import pickle
from copy import deepcopy

if sys.platform == "darwin":  # macOS check
    mp.set_start_method("spawn", force=True)

def QA(question:str, choices:list):
    qa_agent = Agent()
    str_choices = zip(range(len(choices)), choices)
    alpha = "ABCDEFGHIJKLMNOPQRSTUVWXYZ"
    str_choices = "\n".join([f"{alpha[idx]}. {choice}" for idx, choice in str_choices])
    qa_agent.load_message([{"role": "user", "content": f"# Question: \n{question} # Choices: \n{str_choices}"}])
    response_format={
        "type": "json_schema",
        "json_schema": {
        "name": "multiple_choice_response",
        "strict": True,
        "schema": {
            "type": "object",
            "properties": {
            "response": {
                "type": "string",
                "description": "The letter corresponding to the answer.",
                "enum": [alpha[idx] for idx in range(len(choices))]
            }
            },
            "required": [
                "response"
            ],
            "additionalProperties": False
        }
        }
    }
    response_json, response_logprobs = qa_agent.get_response(response_format = response_format, logprobs = True, temperature = 0)
    response = json.loads(response_json)["response"]
    top_prob = top_norm_prob(response_logprobs, response)
    top_prob_list = [(x.token, round(np.exp(x.logprob), 4)) for x in list(response_logprobs[3].top_logprobs)][:len(choices)]
    return response, top_prob, top_prob_list

def process_org(ele):
    # Process a single MMLU sample.
    # takes mmlu data as input
    # output is _org data

    question = ele['question']
    choices = ele['options']
    pred, prob, topk = QA(question, choices)
    ele['r'] = pred
    ele['p_r'] = prob
    ele['topk'] = topk
    return ele  # Return updated sample

def process_one(ele, disagree_type):
    # Process a single MMLU sample.
    # takes _org data as input, which should be generated by process_org()
    # output is _one_rnd data
    if disagree_type == 'rnd:':
        prev_topk = ele['topk']
        prev_r = ele['r']
        r_j_pool = [x for x in prev_topk if x[0] != prev_r]
        r_j_tuple = random.Random(0).choice(r_j_pool)
        r_j = r_j_tuple[0]
        p_r_j = r_j_tuple[1]
    elif disagree_type == '1st':
        r_j = ele['r']
        p_r_j = ele['p_r']
    elif disagree_type == '2nd':
        prev_topk = ele['topk']
        r_j = prev_topk[1][0]
        p_r_j = prev_topk[1][1]
    elif disagree_type == 'lst':
        prev_topk = ele['topk']
        r_j = prev_topk[-1][0]
        p_r_j = prev_topk[-1][1]


    question = ele['question'] + f"# Other's Response:\n Another agent thinks the answer is {r_j}."
    choices = ele['options']
    pred, prob, topk = QA(question, choices)
    ele['r_j'] = r_j
    ele['p_r_j'] = p_r_j
    ele['r'] = pred
    ele['p_r'] = prob
    ele['topk'] = topk
    return ele  # Return updated sample

def process_grp_count(ele, agree_size, disagree_size, disagree_type):
    # Process a single MMLU sample.
    # takes _org data as input, which should be generated by process_org()
    # output is _grp data

    prev_topk = ele['topk']
    R_p = ele['r']
    p_R_p = ele['p_r']
    if disagree_type == "2nd":
        R_n = prev_topk[1][0]
        p_R_n = prev_topk[1][1]

    question = ele['question'] 
    if agree_size + disagree_size > 0:
        question += f"# Other's Response:\n"

    
    if disagree_size > 0:
        question += f"{disagree_size} agents think the answer is {R_n}.\n"
    
    if agree_size > 0:
        question += f"{agree_size} agents think the answer is {R_p}.\n"

    
    choices = ele['options']
    pred, prob, topk = QA(question, choices)
    ele['R_p'] = R_p # positive response from the group
    ele['R_n'] = R_n # negative response from the group
    ele['p_R_p'] = p_R_p
    ele['p_R_n'] = p_R_n
    ele['r'] = pred
    ele['p_r'] = prob
    ele['topk'] = topk
    ele['agree_size'] = agree_size
    ele['disagree_size'] = disagree_size
    ele['disagree_type'] = disagree_type
    return ele  # Return updated sample

def process_grp_count(ele, group_size, disagree_ratio, disagree_type):
    # Process a single MMLU sample.
    # takes _org data as input, which should be generated by process_org()
    # output is _grp data

    prev_topk = ele['topk']
    R_p = ele['r']
    p_R_p = ele['p_r']
    if disagree_type == "2nd":
        R_n = prev_topk[1][0]
        p_R_n = prev_topk[1][1]

    str_disagree_ratio = str(round(disagree_ratio*100))
    str_agree_ratio = str(round((1-disagree_ratio)*100))

    question = ele['question'] 
    question += f"# Other's Response:\n"
    question += f"Among {} agents"
    question += f"{str_disagree_ratio}% of agents think the answer is {R_n}.\n"
    question += f"{str_agree_ratio}% of agents think the answer is {R_p}.\n"

    
    choices = ele['options']
    pred, prob, topk = QA(question, choices)
    ele['R_p'] = R_p # positive response from the group
    ele['R_n'] = R_n # negative response from the group
    ele['p_R_p'] = p_R_p
    ele['p_R_n'] = p_R_n
    ele['r'] = pred
    ele['p_r'] = prob
    ele['topk'] = topk
    ele['group_size'] = group_size
    ele['disagree_ratio'] = disagree_ratio
    ele['disagree_type'] = disagree_type
    return ele  # Return updated sample

def mmlu_eval(mmlu_input, eval_feat, num_workers=mp.cpu_count()):
    """Evaluate MMLU samples using multiprocess for parallel execution with tqdm."""
    eval_type = eval_feat['type']
    mmlu_samples = deepcopy(mmlu_input)

    if eval_type == 'org':
        func = process_org
        input_list = mmlu_samples
    elif eval_type == 'grp_count':
        func = process_grp_count
        agree_size = eval_feat['agree_size']
        disagree_size = eval_feat['disagree_size']
        disagree_type = eval_feat['disagree_type']
        input_list = [(sample, agree_size, disagree_size, disagree_type) for sample in mmlu_samples]
    elif eval_type == 'grp_ratio':

    
    with mp.Pool(num_workers) as pool:
        results = list(tqdm(pool.starmap(func, input_list), total=len(mmlu_samples), desc="Processing MMLU"))

    accuracy = round(sum([x['answer'] == x['r'] for x in results])/len(results),3)
    if eval_type == 'grp_count':
        print(f"Eval Type:{eval_type}, #Agree: {agree_size}, #Disagree: {disagree_size}, Disagree Type: {disagree_type}, Accuracy: {accuracy}")
    else:
        print(f"Eval Type:{eval_type}, Accuracy: {accuracy}")
    
    with open(f"../data/group_da/mmlu_{agree_size}_{disagree_size}_{disagree_type}.pkl", "wb") as f:
        pickle.dump(results, f)
    return results, accuracy  # Return processed samples



